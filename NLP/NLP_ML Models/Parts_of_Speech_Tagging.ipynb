{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Parts_of_Speech_Tagging_v_01.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"dT-2C0JMCgaW"},"source":["**Custom PoS Tagging with scikit-learn**"]},{"cell_type":"code","metadata":{"id":"cZO2OhOkTZoY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623652546765,"user_tz":-330,"elapsed":2268,"user":{"displayName":"Dipanjana Colab","photoUrl":"","userId":"10332801813381954113"}},"outputId":"5a4f431e-80c8-4457-8c82-0ceec51978fc"},"source":["pip install sklearn_crfsuite"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: sklearn_crfsuite in /usr/local/lib/python3.7/dist-packages (0.3.6)\n","Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.7/dist-packages (from sklearn_crfsuite) (4.41.1)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from sklearn_crfsuite) (0.8.9)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sklearn_crfsuite) (1.15.0)\n","Requirement already satisfied: python-crfsuite>=0.8.3 in /usr/local/lib/python3.7/dist-packages (from sklearn_crfsuite) (0.9.7)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qeWHuGt2tUtV"},"source":["import nltk\n","from nltk.corpus import gutenberg as cg\n","from nltk.tokenize import sent_tokenize as st\n","from nltk import word_tokenize,pos_tag\n","import re\n","from nltk.tbl import demo as brill_demo\n","import numpy as np\n","import pandas as pd\n","from subprocess import check_output\n","from nltk.corpus import brown\n","import pprint\n","from sklearn_crfsuite import metrics"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KLSJ8M8uTj1x","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623652550917,"user_tz":-330,"elapsed":1722,"user":{"displayName":"Dipanjana Colab","photoUrl":"","userId":"10332801813381954113"}},"outputId":"8848476e-5d2f-42ff-8841-3c08bf567afd"},"source":["nltk.download('all')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading collection 'all'\n","[nltk_data]    | \n","[nltk_data]    | Downloading package abc to /root/nltk_data...\n","[nltk_data]    |   Package abc is already up-to-date!\n","[nltk_data]    | Downloading package alpino to /root/nltk_data...\n","[nltk_data]    |   Package alpino is already up-to-date!\n","[nltk_data]    | Downloading package biocreative_ppi to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n","[nltk_data]    | Downloading package brown to /root/nltk_data...\n","[nltk_data]    |   Package brown is already up-to-date!\n","[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n","[nltk_data]    |   Package brown_tei is already up-to-date!\n","[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n","[nltk_data]    |   Package cess_cat is already up-to-date!\n","[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n","[nltk_data]    |   Package cess_esp is already up-to-date!\n","[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n","[nltk_data]    |   Package chat80 is already up-to-date!\n","[nltk_data]    | Downloading package city_database to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package city_database is already up-to-date!\n","[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n","[nltk_data]    |   Package cmudict is already up-to-date!\n","[nltk_data]    | Downloading package comparative_sentences to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package comparative_sentences is already up-to-\n","[nltk_data]    |       date!\n","[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n","[nltk_data]    |   Package comtrans is already up-to-date!\n","[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n","[nltk_data]    |   Package conll2000 is already up-to-date!\n","[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n","[nltk_data]    |   Package conll2002 is already up-to-date!\n","[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n","[nltk_data]    |   Package conll2007 is already up-to-date!\n","[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n","[nltk_data]    |   Package crubadan is already up-to-date!\n","[nltk_data]    | Downloading package dependency_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package dependency_treebank is already up-to-date!\n","[nltk_data]    | Downloading package dolch to /root/nltk_data...\n","[nltk_data]    |   Package dolch is already up-to-date!\n","[nltk_data]    | Downloading package europarl_raw to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package europarl_raw is already up-to-date!\n","[nltk_data]    | Downloading package floresta to /root/nltk_data...\n","[nltk_data]    |   Package floresta is already up-to-date!\n","[nltk_data]    | Downloading package framenet_v15 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package framenet_v15 is already up-to-date!\n","[nltk_data]    | Downloading package framenet_v17 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package framenet_v17 is already up-to-date!\n","[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n","[nltk_data]    |   Package gazetteers is already up-to-date!\n","[nltk_data]    | Downloading package genesis to /root/nltk_data...\n","[nltk_data]    |   Package genesis is already up-to-date!\n","[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]    |   Package gutenberg is already up-to-date!\n","[nltk_data]    | Downloading package ieer to /root/nltk_data...\n","[nltk_data]    |   Package ieer is already up-to-date!\n","[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n","[nltk_data]    |   Package inaugural is already up-to-date!\n","[nltk_data]    | Downloading package indian to /root/nltk_data...\n","[nltk_data]    |   Package indian is already up-to-date!\n","[nltk_data]    | Downloading package jeita to /root/nltk_data...\n","[nltk_data]    |   Package jeita is already up-to-date!\n","[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n","[nltk_data]    |   Package kimmo is already up-to-date!\n","[nltk_data]    | Downloading package knbc to /root/nltk_data...\n","[nltk_data]    |   Package knbc is already up-to-date!\n","[nltk_data]    | Downloading package lin_thesaurus to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n","[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n","[nltk_data]    |   Package mac_morpho is already up-to-date!\n","[nltk_data]    | Downloading package machado to /root/nltk_data...\n","[nltk_data]    |   Package machado is already up-to-date!\n","[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n","[nltk_data]    |   Package masc_tagged is already up-to-date!\n","[nltk_data]    | Downloading package moses_sample to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package moses_sample is already up-to-date!\n","[nltk_data]    | Downloading package movie_reviews to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package movie_reviews is already up-to-date!\n","[nltk_data]    | Downloading package names to /root/nltk_data...\n","[nltk_data]    |   Package names is already up-to-date!\n","[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n","[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n","[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n","[nltk_data]    |   Package nps_chat is already up-to-date!\n","[nltk_data]    | Downloading package omw to /root/nltk_data...\n","[nltk_data]    |   Package omw is already up-to-date!\n","[nltk_data]    | Downloading package opinion_lexicon to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n","[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n","[nltk_data]    |   Package paradigms is already up-to-date!\n","[nltk_data]    | Downloading package pil to /root/nltk_data...\n","[nltk_data]    |   Package pil is already up-to-date!\n","[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n","[nltk_data]    |   Package pl196x is already up-to-date!\n","[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n","[nltk_data]    |   Package ppattach is already up-to-date!\n","[nltk_data]    | Downloading package problem_reports to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package problem_reports is already up-to-date!\n","[nltk_data]    | Downloading package propbank to /root/nltk_data...\n","[nltk_data]    |   Package propbank is already up-to-date!\n","[nltk_data]    | Downloading package ptb to /root/nltk_data...\n","[nltk_data]    |   Package ptb is already up-to-date!\n","[nltk_data]    | Downloading package product_reviews_1 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n","[nltk_data]    | Downloading package product_reviews_2 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n","[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n","[nltk_data]    |   Package pros_cons is already up-to-date!\n","[nltk_data]    | Downloading package qc to /root/nltk_data...\n","[nltk_data]    |   Package qc is already up-to-date!\n","[nltk_data]    | Downloading package reuters to /root/nltk_data...\n","[nltk_data]    |   Package reuters is already up-to-date!\n","[nltk_data]    | Downloading package rte to /root/nltk_data...\n","[nltk_data]    |   Package rte is already up-to-date!\n","[nltk_data]    | Downloading package semcor to /root/nltk_data...\n","[nltk_data]    |   Package semcor is already up-to-date!\n","[nltk_data]    | Downloading package senseval to /root/nltk_data...\n","[nltk_data]    |   Package senseval is already up-to-date!\n","[nltk_data]    | Downloading package sentiwordnet to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package sentiwordnet is already up-to-date!\n","[nltk_data]    | Downloading package sentence_polarity to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package sentence_polarity is already up-to-date!\n","[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n","[nltk_data]    |   Package shakespeare is already up-to-date!\n","[nltk_data]    | Downloading package sinica_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package sinica_treebank is already up-to-date!\n","[nltk_data]    | Downloading package smultron to /root/nltk_data...\n","[nltk_data]    |   Package smultron is already up-to-date!\n","[nltk_data]    | Downloading package state_union to /root/nltk_data...\n","[nltk_data]    |   Package state_union is already up-to-date!\n","[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n","[nltk_data]    |   Package stopwords is already up-to-date!\n","[nltk_data]    | Downloading package subjectivity to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package subjectivity is already up-to-date!\n","[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n","[nltk_data]    |   Package swadesh is already up-to-date!\n","[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n","[nltk_data]    |   Package switchboard is already up-to-date!\n","[nltk_data]    | Downloading package timit to /root/nltk_data...\n","[nltk_data]    |   Package timit is already up-to-date!\n","[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n","[nltk_data]    |   Package toolbox is already up-to-date!\n","[nltk_data]    | Downloading package treebank to /root/nltk_data...\n","[nltk_data]    |   Package treebank is already up-to-date!\n","[nltk_data]    | Downloading package twitter_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package twitter_samples is already up-to-date!\n","[nltk_data]    | Downloading package udhr to /root/nltk_data...\n","[nltk_data]    |   Package udhr is already up-to-date!\n","[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n","[nltk_data]    |   Package udhr2 is already up-to-date!\n","[nltk_data]    | Downloading package unicode_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package unicode_samples is already up-to-date!\n","[nltk_data]    | Downloading package universal_treebanks_v20 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n","[nltk_data]    |       date!\n","[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n","[nltk_data]    |   Package verbnet is already up-to-date!\n","[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n","[nltk_data]    |   Package verbnet3 is already up-to-date!\n","[nltk_data]    | Downloading package webtext to /root/nltk_data...\n","[nltk_data]    |   Package webtext is already up-to-date!\n","[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n","[nltk_data]    |   Package wordnet is already up-to-date!\n","[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n","[nltk_data]    |   Package wordnet_ic is already up-to-date!\n","[nltk_data]    | Downloading package words to /root/nltk_data...\n","[nltk_data]    |   Package words is already up-to-date!\n","[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n","[nltk_data]    |   Package ycoe is already up-to-date!\n","[nltk_data]    | Downloading package rslp to /root/nltk_data...\n","[nltk_data]    |   Package rslp is already up-to-date!\n","[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n","[nltk_data]    |       to-date!\n","[nltk_data]    | Downloading package universal_tagset to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package universal_tagset is already up-to-date!\n","[nltk_data]    | Downloading package maxent_ne_chunker to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n","[nltk_data]    | Downloading package punkt to /root/nltk_data...\n","[nltk_data]    |   Package punkt is already up-to-date!\n","[nltk_data]    | Downloading package book_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package book_grammars is already up-to-date!\n","[nltk_data]    | Downloading package sample_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package sample_grammars is already up-to-date!\n","[nltk_data]    | Downloading package spanish_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package spanish_grammars is already up-to-date!\n","[nltk_data]    | Downloading package basque_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package basque_grammars is already up-to-date!\n","[nltk_data]    | Downloading package large_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package large_grammars is already up-to-date!\n","[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n","[nltk_data]    |   Package tagsets is already up-to-date!\n","[nltk_data]    | Downloading package snowball_data to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package snowball_data is already up-to-date!\n","[nltk_data]    | Downloading package bllip_wsj_no_aux to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n","[nltk_data]    | Downloading package word2vec_sample to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package word2vec_sample is already up-to-date!\n","[nltk_data]    | Downloading package panlex_swadesh to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n","[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n","[nltk_data]    |   Package mte_teip5 is already up-to-date!\n","[nltk_data]    | Downloading package averaged_perceptron_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n","[nltk_data]    |       to-date!\n","[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n","[nltk_data]    |       up-to-date!\n","[nltk_data]    | Downloading package perluniprops to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package perluniprops is already up-to-date!\n","[nltk_data]    | Downloading package nonbreaking_prefixes to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n","[nltk_data]    | Downloading package vader_lexicon to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package vader_lexicon is already up-to-date!\n","[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n","[nltk_data]    |   Package porter_test is already up-to-date!\n","[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n","[nltk_data]    |   Package wmt15_eval is already up-to-date!\n","[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n","[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n","[nltk_data]    | \n","[nltk_data]  Done downloading collection all\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"8u8SNMkUTv4-"},"source":["sentence = \"the little brown dog barked at the cat\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HFC24CxhTvqd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623652553981,"user_tz":-330,"elapsed":10,"user":{"displayName":"Dipanjana Colab","photoUrl":"","userId":"10332801813381954113"}},"outputId":"34b32a89-168b-4b3c-9ef3-dd611856854b"},"source":["tagged = nltk.pos_tag(['I',  'watched',  'the', 'first',  'early','morning',  'sunbeams', 'creeping',  'through',  'the', 'Green','Forest'])\n","tagged"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('I', 'PRP'),\n"," ('watched', 'VBD'),\n"," ('the', 'DT'),\n"," ('first', 'JJ'),\n"," ('early', 'JJ'),\n"," ('morning', 'NN'),\n"," ('sunbeams', 'NN'),\n"," ('creeping', 'VBG'),\n"," ('through', 'IN'),\n"," ('the', 'DT'),\n"," ('Green', 'NNP'),\n"," ('Forest', 'NNP')]"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"CvLBfA23CdL2","executionInfo":{"status":"ok","timestamp":1623652912122,"user_tz":-330,"elapsed":364,"user":{"displayName":"Dipanjana Colab","photoUrl":"","userId":"10332801813381954113"}}},"source":["# Data collection\n","tagged_sentences = nltk.corpus.treebank.tagged_sents()\n","\n","# Function for detailed feature extraction\n","def features(sentence, index):\n","    \"\"\" sentence: [w1, w2, ...], index: the index of the word \"\"\"\n","    return {\n","        'word': sentence[index],\n","        'is_first': index == 0,\n","        'is_last': index == len(sentence) - 1,\n","        'is_capitalized': sentence[index][0].upper() == sentence[index][0],\n","        'is_all_caps': sentence[index].upper() == sentence[index],\n","        'is_all_lower': sentence[index].lower() == sentence[index],\n","        'prefix-1': sentence[index][0],\n","        'prefix-2': sentence[index][:2],\n","        'prefix-3': sentence[index][:3],\n","        'suffix-1': sentence[index][-1],\n","        'suffix-2': sentence[index][-2:],\n","        'suffix-3': sentence[index][-3:],\n","        'prev_word': '' if index == 0 else sentence[index - 1],\n","        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n","        'has_hyphen': '-' in sentence[index],\n","        'is_numeric': sentence[index].isdigit(),\n","        'capitals_inside': sentence[index][1:].lower() != sentence[index][1:]\n","    }\n"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"4uYdR4EsFt7r","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623652913865,"user_tz":-330,"elapsed":312,"user":{"displayName":"Dipanjana Colab","photoUrl":"","userId":"10332801813381954113"}},"outputId":"00cfb6ff-1fc5-4504-e2a9-18d7de7c2e27"},"source":["pprint.pprint(features(['This', 'is', 'Dipanjana'], 2))\n"],"execution_count":23,"outputs":[{"output_type":"stream","text":["{'capitals_inside': False,\n"," 'has_hyphen': False,\n"," 'is_all_caps': False,\n"," 'is_all_lower': False,\n"," 'is_capitalized': True,\n"," 'is_first': False,\n"," 'is_last': True,\n"," 'is_numeric': False,\n"," 'next_word': '',\n"," 'prefix-1': 'D',\n"," 'prefix-2': 'Di',\n"," 'prefix-3': 'Dip',\n"," 'prev_word': 'is',\n"," 'suffix-1': 'a',\n"," 'suffix-2': 'na',\n"," 'suffix-3': 'ana',\n"," 'word': 'Dipanjana'}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GNR6LvBdFusK","executionInfo":{"status":"ok","timestamp":1623652915280,"user_tz":-330,"elapsed":7,"user":{"displayName":"Dipanjana Colab","photoUrl":"","userId":"10332801813381954113"}}},"source":["# Helper function to remove tag from each word to make the data set prepared for custom PoS tagger building\n","def untag(tagged_sentence):\n","    return [w for w, t in tagged_sentence]"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"pexxPR01xP93","executionInfo":{"status":"ok","timestamp":1623652916766,"user_tz":-330,"elapsed":1044,"user":{"displayName":"Dipanjana Colab","photoUrl":"","userId":"10332801813381954113"}}},"source":["# Split the dataset for training and testing\n","cutoff = int(.75 * len(tagged_sentences))\n","training_sentences = tagged_sentences[:cutoff]\n","test_sentences = tagged_sentences[cutoff:]"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"GkcpwEcOFvTd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623655464786,"user_tz":-330,"elapsed":1350,"user":{"displayName":"Dipanjana Colab","photoUrl":"","userId":"10332801813381954113"}},"outputId":"d4b5ef02-8431-4670-caa7-7230e8eecb7e"},"source":["def transform_to_dataset(tagged_sentences):\n","    X, y = [], []\n"," \n","    for tagged in tagged_sentences:\n","        for index in range(len(tagged)):\n","            X.append(features(untag(tagged), index))\n","            y.append(tagged[index][1])\n"," \n","    return X, y\n"," \n","X, y = transform_to_dataset(training_sentences)\n","X[1], y[1]"],"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["({'capitals_inside': False,\n","  'has_hyphen': False,\n","  'is_all_caps': False,\n","  'is_all_lower': False,\n","  'is_capitalized': True,\n","  'is_first': False,\n","  'is_last': False,\n","  'is_numeric': False,\n","  'next_word': ',',\n","  'prefix-1': 'V',\n","  'prefix-2': 'Vi',\n","  'prefix-3': 'Vin',\n","  'prev_word': 'Pierre',\n","  'suffix-1': 'n',\n","  'suffix-2': 'en',\n","  'suffix-3': 'ken',\n","  'word': 'Vinken'},\n"," 'NNP')"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"id":"jly6ij6Dq8Dd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623655877145,"user_tz":-330,"elapsed":411166,"user":{"displayName":"Dipanjana Colab","photoUrl":"","userId":"10332801813381954113"}},"outputId":"6f9fe86e-aec9-4a98-88e0-0fead854aab7"},"source":["# Training the PoS Tagger with DecisionTreeClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.feature_extraction import DictVectorizer\n","from sklearn.pipeline import Pipeline\n","\n","clf = Pipeline([\n","    ('vectorizer', DictVectorizer(sparse=False)),\n","    ('classifier', DecisionTreeClassifier(criterion='entropy'))\n","])\n","clf.fit(X, y)"],"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Pipeline(memory=None,\n","         steps=[('vectorizer',\n","                 DictVectorizer(dtype=<class 'numpy.float64'>, separator='=',\n","                                sort=True, sparse=False)),\n","                ('classifier',\n","                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,\n","                                        criterion='entropy', max_depth=None,\n","                                        max_features=None, max_leaf_nodes=None,\n","                                        min_impurity_decrease=0.0,\n","                                        min_impurity_split=None,\n","                                        min_samples_leaf=1, min_samples_split=2,\n","                                        min_weight_fraction_leaf=0.0,\n","                                        presort='deprecated', random_state=None,\n","                                        splitter='best'))],\n","         verbose=False)"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"nnPE86mlHrVi","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1602692167402,"user_tz":-60,"elapsed":7422,"user":{"displayName":"Dipanjana Colab","photoUrl":"","userId":"10332801813381954113"}},"outputId":"72881a78-8652-4966-9415-43475eae87de"},"source":["# Validating on test dataset\n","X_test, y_test = transform_to_dataset(test_sentences)\n"," \n","print (\"Accuracy:\", clf.score(X_test, y_test))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy: 0.9451631046119235\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yHCssW32HsVl"},"source":["# Validating on new/ random sentences\n","\n","# Helper function to break the input sentence into tokens and tag each token using the trained model\n","def pos_tag(sentence):\n","  tags = clf.predict([features(sentence, index) for index in range(len(sentence))])\n","  #return zip(sentence, tags)\n","  return (sentence, tags)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JRn44lQ5Hs06","colab":{"base_uri":"https://localhost:8080/","height":612},"executionInfo":{"status":"ok","timestamp":1602694788792,"user_tz":-60,"elapsed":547,"user":{"displayName":"Dipanjana Colab","photoUrl":"","userId":"10332801813381954113"}},"outputId":"eee578ec-d97e-42f5-daf4-ae13f8f7d57d"},"source":["# Validation sentence 1\n","test_sentence = ['Buster',  'Bear',  'yawned',  'as',  'he',  'lay',  'on',  'his',  'comfortable',  'bed',  'of',  'leaves',  'and',  'watched',  'the', 'first',  'early','morning',  'sunbeams', 'creeping',  'through',  'the', 'Green','Forest',  'to',  'chase',  'out',  'the','Black','Shadows','.']\n","\n","pos_tag(test_sentence)\n","\n","# O/P: Unsatisfactory\n","# Mistakes in the above output:\n","# 1. 'the black shadow' -- 'black' tagged as 'NNP' i.e proper noun where it is 'JJ' i.e Adjective\n","# 2. 'first early morning sunbeams' -- as per the context 'morning' should be 'JJ' but tagged as 'NN' and accordingly\\\n","# 'first' and 'early' should be adverb (RB) but wrongly tagged as 'JJ'(adjective)\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(['Buster',\n","  'Bear',\n","  'yawned',\n","  'as',\n","  'he',\n","  'lay',\n","  'on',\n","  'his',\n","  'comfortable',\n","  'bed',\n","  'of',\n","  'leaves',\n","  'and',\n","  'watched',\n","  'the',\n","  'first',\n","  'early',\n","  'morning',\n","  'sunbeams',\n","  'creeping',\n","  'through',\n","  'the',\n","  'Green',\n","  'Forest',\n","  'to',\n","  'chase',\n","  'out',\n","  'the',\n","  'Black',\n","  'Shadows',\n","  '.'],\n"," array(['NNP', 'NN', 'VBD', 'IN', 'PRP', 'VBD', 'IN', 'PRP$', 'JJ', 'VBD',\n","        'IN', 'NNS', 'CC', 'VBD', 'DT', 'JJ', 'JJ', 'NN', 'NNS', 'VBG',\n","        'IN', 'DT', 'NNP', 'NNP', 'TO', 'VB', 'RP', 'DT', 'NNP', 'NNPS',\n","        '.'], dtype='<U6'))"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"WjnnKtbVW7hc","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1571226128950,"user_tz":-60,"elapsed":681,"user":{"displayName":"Dipanjana Colab","photoUrl":"","userId":"10332801813381954113"}},"outputId":"9a727405-c7ce-4559-db4b-8a1c624358df"},"source":["# Validation sentence 2\n","pos_tag(word_tokenize('we lost tickets to this lost game'))\n","\n","# O/P: Unsatisfactory\n","# Mistake:\n","# 2nd 'lost' should be 'JJ' but tagged wrongly as 'VBD'"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(['we', 'lost', 'tickets', 'to', 'this', 'lost', 'game'],\n"," array(['PRP', 'VBD', 'NNS', 'TO', 'DT', 'VBD', 'NN'], dtype='<U6'))"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"markdown","metadata":{"id":"bCEIdzZXaxQd"},"source":["**POS tagger using a Conditional Random Field**"]},{"cell_type":"code","metadata":{"id":"lnvhmCFqarfK","colab":{"base_uri":"https://localhost:8080/","height":105},"executionInfo":{"status":"ok","timestamp":1571233714677,"user_tz":-60,"elapsed":1852,"user":{"displayName":"Dipanjana Colab","photoUrl":"","userId":"10332801813381954113"}},"outputId":"445d1156-e4b1-49be-94be-01c9774b89b4"},"source":["from nltk.tag.util import untag\n"," \n","# Split the dataset for training and testing\n","cutoff = int(.75 * len(tagged_sentences))\n","training_sentences = tagged_sentences[:cutoff]\n","test_sentences = tagged_sentences[cutoff:]\n","\n","def transform_to_dataset(tagged_sentences):\n","    X, y = [], []\n"," \n","    for tagged in tagged_sentences:\n","        X.append([features(untag(tagged), index) for index in range(len(tagged))])\n","        y.append([tag for _, tag in tagged])\n"," \n","    return X, y\n"," \n","X_train, y_train = transform_to_dataset(training_sentences)\n","X_test, y_test = transform_to_dataset(test_sentences)\n"," \n","print(len(X_train))     \n","print(len(X_test))         \n","print(X_train[0])\n","print(y_train[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2935\n","979\n","[{'word': 'Pierre', 'is_first': True, 'is_last': False, 'is_capitalized': True, 'is_all_caps': False, 'is_all_lower': False, 'prefix-1': 'P', 'prefix-2': 'Pi', 'prefix-3': 'Pie', 'suffix-1': 'e', 'suffix-2': 're', 'suffix-3': 'rre', 'prev_word': '', 'next_word': 'Vinken', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'Vinken', 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': False, 'is_all_lower': False, 'prefix-1': 'V', 'prefix-2': 'Vi', 'prefix-3': 'Vin', 'suffix-1': 'n', 'suffix-2': 'en', 'suffix-3': 'ken', 'prev_word': 'Pierre', 'next_word': ',', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': ',', 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': True, 'prefix-1': ',', 'prefix-2': ',', 'prefix-3': ',', 'suffix-1': ',', 'suffix-2': ',', 'suffix-3': ',', 'prev_word': 'Vinken', 'next_word': '61', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': '61', 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': True, 'prefix-1': '6', 'prefix-2': '61', 'prefix-3': '61', 'suffix-1': '1', 'suffix-2': '61', 'suffix-3': '61', 'prev_word': ',', 'next_word': 'years', 'has_hyphen': False, 'is_numeric': True, 'capitals_inside': False}, {'word': 'years', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 'y', 'prefix-2': 'ye', 'prefix-3': 'yea', 'suffix-1': 's', 'suffix-2': 'rs', 'suffix-3': 'ars', 'prev_word': '61', 'next_word': 'old', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'old', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 'o', 'prefix-2': 'ol', 'prefix-3': 'old', 'suffix-1': 'd', 'suffix-2': 'ld', 'suffix-3': 'old', 'prev_word': 'years', 'next_word': ',', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': ',', 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': True, 'prefix-1': ',', 'prefix-2': ',', 'prefix-3': ',', 'suffix-1': ',', 'suffix-2': ',', 'suffix-3': ',', 'prev_word': 'old', 'next_word': 'will', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'will', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 'w', 'prefix-2': 'wi', 'prefix-3': 'wil', 'suffix-1': 'l', 'suffix-2': 'll', 'suffix-3': 'ill', 'prev_word': ',', 'next_word': 'join', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'join', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 'j', 'prefix-2': 'jo', 'prefix-3': 'joi', 'suffix-1': 'n', 'suffix-2': 'in', 'suffix-3': 'oin', 'prev_word': 'will', 'next_word': 'the', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'the', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 't', 'prefix-2': 'th', 'prefix-3': 'the', 'suffix-1': 'e', 'suffix-2': 'he', 'suffix-3': 'the', 'prev_word': 'join', 'next_word': 'board', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'board', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 'b', 'prefix-2': 'bo', 'prefix-3': 'boa', 'suffix-1': 'd', 'suffix-2': 'rd', 'suffix-3': 'ard', 'prev_word': 'the', 'next_word': 'as', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'as', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 'a', 'prefix-2': 'as', 'prefix-3': 'as', 'suffix-1': 's', 'suffix-2': 'as', 'suffix-3': 'as', 'prev_word': 'board', 'next_word': 'a', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'a', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 'a', 'prefix-2': 'a', 'prefix-3': 'a', 'suffix-1': 'a', 'suffix-2': 'a', 'suffix-3': 'a', 'prev_word': 'as', 'next_word': 'nonexecutive', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'nonexecutive', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 'n', 'prefix-2': 'no', 'prefix-3': 'non', 'suffix-1': 'e', 'suffix-2': 've', 'suffix-3': 'ive', 'prev_word': 'a', 'next_word': 'director', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'director', 'is_first': False, 'is_last': False, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': True, 'prefix-1': 'd', 'prefix-2': 'di', 'prefix-3': 'dir', 'suffix-1': 'r', 'suffix-2': 'or', 'suffix-3': 'tor', 'prev_word': 'nonexecutive', 'next_word': 'Nov.', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': 'Nov.', 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': False, 'is_all_lower': False, 'prefix-1': 'N', 'prefix-2': 'No', 'prefix-3': 'Nov', 'suffix-1': '.', 'suffix-2': 'v.', 'suffix-3': 'ov.', 'prev_word': 'director', 'next_word': '29', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}, {'word': '29', 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': True, 'prefix-1': '2', 'prefix-2': '29', 'prefix-3': '29', 'suffix-1': '9', 'suffix-2': '29', 'suffix-3': '29', 'prev_word': 'Nov.', 'next_word': '.', 'has_hyphen': False, 'is_numeric': True, 'capitals_inside': False}, {'word': '.', 'is_first': False, 'is_last': True, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': True, 'prefix-1': '.', 'prefix-2': '.', 'prefix-3': '.', 'suffix-1': '.', 'suffix-2': '.', 'suffix-3': '.', 'prev_word': '29', 'next_word': '', 'has_hyphen': False, 'is_numeric': False, 'capitals_inside': False}]\n","['NNP', 'NNP', ',', 'CD', 'NNS', 'JJ', ',', 'MD', 'VB', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NNP', 'CD', '.']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"26Xy0-LD1CRU","colab":{"base_uri":"https://localhost:8080/","height":207},"executionInfo":{"status":"ok","timestamp":1571233766330,"user_tz":-60,"elapsed":4526,"user":{"displayName":"Dipanjana Colab","photoUrl":"","userId":"10332801813381954113"}},"outputId":"20af3f66-b8dd-43ee-f4b3-d24ef3b4f9f1"},"source":["pip install sklearn-crfsuite"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting sklearn-crfsuite\n","  Downloading https://files.pythonhosted.org/packages/25/74/5b7befa513482e6dee1f3dd68171a6c9dfc14c0eaa00f885ffeba54fe9b0/sklearn_crfsuite-0.3.6-py2.py3-none-any.whl\n","Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.6/dist-packages (from sklearn-crfsuite) (4.28.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sklearn-crfsuite) (1.12.0)\n","Collecting python-crfsuite>=0.8.3 (from sklearn-crfsuite)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2f/86/cfcd71edca9d25d3d331209a20f6314b6f3f134c29478f90559cee9ce091/python_crfsuite-0.9.6-cp36-cp36m-manylinux1_x86_64.whl (754kB)\n","\u001b[K     |████████████████████████████████| 757kB 5.2MB/s \n","\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from sklearn-crfsuite) (0.8.5)\n","Installing collected packages: python-crfsuite, sklearn-crfsuite\n","Successfully installed python-crfsuite-0.9.6 sklearn-crfsuite-0.3.6\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KxlZQkABaviB","colab":{"base_uri":"https://localhost:8080/","height":136},"executionInfo":{"status":"ok","timestamp":1571233839703,"user_tz":-60,"elapsed":72030,"user":{"displayName":"Dipanjana Colab","photoUrl":"","userId":"10332801813381954113"}},"outputId":"bf9e55a1-0afb-4aac-d5e6-6a216afb66d6"},"source":["from sklearn_crfsuite import CRF\n","\n","model = CRF()\n","model.fit(X_train, y_train)\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["CRF(algorithm=None, all_possible_states=None, all_possible_transitions=None,\n","    averaging=None, c=None, c1=None, c2=None, calibration_candidates=None,\n","    calibration_eta=None, calibration_max_trials=None, calibration_rate=None,\n","    calibration_samples=None, delta=None, epsilon=None, error_sensitive=None,\n","    gamma=None, keep_tempfiles=None, linesearch=None, max_iterations=None,\n","    max_linesearch=None, min_freq=None, model_filename=None, num_memories=None,\n","    pa_type=None, period=None, trainer_cls=None, variance=None, verbose=False)"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"wW7OoImo5_fX"},"source":["# Helper function to break the input sentence into tokens and tag each token using the trained model\n","def pos_tag(sentence):\n","    sentence_features = [features(sentence, index) for index in range(len(sentence))]\n","    return list((sentence, model.predict([sentence_features])[0]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cqey9q_F8MdB","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1571235634131,"user_tz":-60,"elapsed":869,"user":{"displayName":"Dipanjana Colab","photoUrl":"","userId":"10332801813381954113"}},"outputId":"91632d0e-4d1f-4ba8-f362-bbc1d413420e"},"source":["# Validation of model performance\n"," \n","y_pred = model.predict(X_test)\n","print(metrics.flat_accuracy_score(y_test, y_pred))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.9602683593122289\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tM30RdnPbqcC","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1571235171021,"user_tz":-60,"elapsed":385,"user":{"displayName":"Dipanjana Colab","photoUrl":"","userId":"10332801813381954113"}},"outputId":"31e216ee-b0bc-4801-9001-f1eb87278fa4"},"source":["# Validation sentence 1\n","sentence = word_tokenize('bob made a book collector happy the other day')\n"," \n","print(pos_tag(sentence))  \n","\n","# O/P: Wrong\n","# # 'book' should be adjective but tagged as 'NN' (Noun)\n","# 'happy' should be adverb but tagged as 'IN' (Preposition or subordinating conjunction)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[['bob', 'made', 'a', 'book', 'collector', 'happy', 'the', 'other', 'day'], ['NN', 'VBD', 'DT', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN']]\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["['bob', 'made', 'a', 'book', 'collector', 'happy', 'the', 'other', 'day']"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"9il_f16M3UrE","colab":{"base_uri":"https://localhost:8080/","height":71},"executionInfo":{"status":"ok","timestamp":1571235571894,"user_tz":-60,"elapsed":343,"user":{"displayName":"Dipanjana Colab","photoUrl":"","userId":"10332801813381954113"}},"outputId":"fdee6a5f-3040-4e33-a260-a1fa1be7c0f8"},"source":["# Validation sentence 2\n","sentence = word_tokenize('we lost tickets to this lost game')\n"," \n","print(pos_tag(sentence)) \n","# O/P: Wrong\n","\n","# Validation sentence 3\n","\n","sentence = ['Buster',\n","  'Bear',\n","  'yawned',\n","  'as',\n","  'he',\n","  'lay',\n","  'on',\n","  'his',\n","  'comfortable',\n","  'bed',\n","  'of',\n","  'leaves',\n","  'and',\n","  'watched',\n","  'the',\n","  'first',\n","  'early',\n","  'morning',\n","  'sunbeams',\n","  'creeping',\n","  'through',\n","  'the',\n","  'Green',\n","  'Forest',\n","  'to',\n","  'chase',\n","  'out',\n","  'the',\n","  'Black',\n","  'Shadows',\n","  '.']\n","\n","print(pos_tag(sentence)) \n","# O/P: Wrong"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[['we', 'lost', 'tickets', 'to', 'this', 'lost', 'game'], ['PRP', 'VBD', 'NNS', 'TO', 'DT', 'NN', 'NN']]\n","[['Buster', 'Bear', 'yawned', 'as', 'he', 'lay', 'on', 'his', 'comfortable', 'bed', 'of', 'leaves', 'and', 'watched', 'the', 'first', 'early', 'morning', 'sunbeams', 'creeping', 'through', 'the', 'Green', 'Forest', 'to', 'chase', 'out', 'the', 'Black', 'Shadows', '.'], ['NNP', 'NNP', 'VBD', 'IN', 'PRP', 'VBP', 'IN', 'PRP$', 'NN', 'VBD', 'IN', 'NNS', 'CC', 'VBD', 'DT', 'JJ', 'JJ', 'NN', 'NNS', 'VBG', 'IN', 'DT', 'NNP', 'NNP', 'TO', 'VB', 'RP', 'DT', 'NNP', 'NNP', '.']]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"B4-ZIZTEFpM8"},"source":["**Auto Tagging**"]},{"cell_type":"code","metadata":{"id":"gl9sbfx_brjp","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1571238776908,"user_tz":-60,"elapsed":1388,"user":{"displayName":"Dipanjana Colab","photoUrl":"","userId":"10332801813381954113"}},"outputId":"9fcdcf6e-e49f-4417-9cf6-ac23cc503950"},"source":["#tags = [tag for (word, tag) in tagged_sentences]\n","#print(\"Most common tag is : \", nltk.FreqDist(tags).max())\n","\n","#tagged_sentences\n","\n","for tagged_sentence in tagged_sentences:\n","  tags = [tag for (word, tag) in tagged_sentence]\n","\n","print(\"Most common tag is : \", nltk.FreqDist(tags).max())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Most common tag is :  NN\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vCvQ-MWiAt6W"},"source":["# Working with another data set : Brown Corpus\n","brown_tagged_sents = brown.tagged_sents(categories='news')\n","brown_sents = brown.sents(categories='news')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XTjZPR8jLJ9q","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1571239638236,"user_tz":-60,"elapsed":537,"user":{"displayName":"Dipanjana Colab","photoUrl":"","userId":"10332801813381954113"}},"outputId":"7b1c6603-53da-4279-9bf4-2b46141c2a95"},"source":["tags = [tag for (word, tag) in brown.tagged_words(categories='news')]\n","print(\"Most common tag is : \", nltk.FreqDist(tags).max())\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Most common tag is :  NN\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lcxbw6kaLKeP","colab":{"base_uri":"https://localhost:8080/","height":105},"executionInfo":{"status":"ok","timestamp":1571239791100,"user_tz":-60,"elapsed":780,"user":{"displayName":"Dipanjana Colab","photoUrl":"","userId":"10332801813381954113"}},"outputId":"5f133d3e-9e7f-4f9a-cfde-414cf7cd8402"},"source":["# Default Tagger\n","\n","default_tagger = nltk.DefaultTagger('NN')\n","print(\"\\nCheck results : \", default_tagger.tag(word_tokenize('bob made a book collector happy the other day')))\n","\n","# Performances : \n","print(\"\\nPerformance with default tagger : \", default_tagger.evaluate(brown_tagged_sents))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Check results :  [('bob', 'NN'), ('made', 'NN'), ('a', 'NN'), ('book', 'NN'), ('collector', 'NN'), ('happy', 'NN'), ('the', 'NN'), ('other', 'NN'), ('day', 'NN')]\n","\n","Performance with default tagger :  0.13089484257215028\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VRUhG6U3LLBj","colab":{"base_uri":"https://localhost:8080/","height":646},"executionInfo":{"status":"ok","timestamp":1571240293167,"user_tz":-60,"elapsed":400,"user":{"displayName":"Dipanjana Colab","photoUrl":"","userId":"10332801813381954113"}},"outputId":"61445b6d-4247-4811-bcb7-f4fd778f1a2c"},"source":["# Regex Tagging\n","\n","# Regex pattern\n","patterns = [\n","    (r'.*ing$', 'VBG'),               # gerunds\n","    (r'.*ed$', 'VBD'),                # simple past\n","    (r'.*es$', 'VBZ'),                # 3rd singular present\n","    (r'.*ould$', 'MD'),               # modals\n","    (r'.*\\'s$', 'NN$'),               # possessive nouns\n","    (r'.*s$', 'NNS'),                 # plural nouns\n","    (r'^-?[0-9]+(.[0-9]+)?$', 'CD'),  # cardinal numbers\n","    (r'(The|the|A|a|An|an)$', 'AT'),   # articles \n","    (r'.*able$', 'JJ'),                # adjectives \n","    (r'.*ness$', 'NN'),                # nouns formed from adjectives\n","    (r'.*ly$', 'RB'),                  # adverbs\n","    (r'(He|he|She|she|It|it|I|me|Me|You|you)$', 'PRP'), # pronouns\n","    (r'(His|his|Her|her|Its|its)$', 'PRP$'),    # possesive\n","    (r'(my|Your|your|Yours|yours)$', 'PRP$'),   # possesive\n","    (r'(on|On|in|In|at|At|since|Since)$', 'IN'),# time prepopsitions\n","    (r'(for|For|ago|Ago|before|Before)$', 'IN'),# time prepopsitions\n","    (r'(till|Till|until|Until)$', 'IN'),        # time prepopsitions\n","    (r'(by|By|beside|Beside)$', 'IN'),          # space prepopsitions\n","    (r'(under|Under|below|Below)$', 'IN'),      # space prepopsitions\n","    (r'(over|Over|above|Above)$', 'IN'),        # space prepopsitions\n","    (r'(across|Across|through|Through)$', 'IN'),# space prepopsitions\n","    (r'(into|Into|towards|Towards)$', 'IN'),    # space prepopsitions\n","    (r'(onto|Onto|from|From)$', 'IN'),          # space prepopsitions    \n","    (r'\\.$','.'), (r'\\,$',','), (r'\\?$','?'),    # fullstop, comma, Qmark\n","    (r'\\($','('), (r'\\)$',')'),             # round brackets\n","    (r'\\[$','['), (r'\\]$',']'),             # square brackets\n","    (r'(Sam)$', 'NAM'),\n","    # WARNING : Put the default value in the end\n","    (r'.*', 'NN')                      # nouns (default)\n","]\n","\n","# Construct Tagger\n","regexp_tagger = nltk.RegexpTagger(patterns)\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['``',\n"," 'Only',\n"," 'a',\n"," 'relative',\n"," 'handful',\n"," 'of',\n"," 'such',\n"," 'reports',\n"," 'was',\n"," 'received',\n"," \"''\",\n"," ',',\n"," 'the',\n"," 'jury',\n"," 'said',\n"," ',',\n"," '``',\n"," 'considering',\n"," 'the',\n"," 'widespread',\n"," 'interest',\n"," 'in',\n"," 'the',\n"," 'election',\n"," ',',\n"," 'the',\n"," 'number',\n"," 'of',\n"," 'voters',\n"," 'and',\n"," 'the',\n"," 'size',\n"," 'of',\n"," 'this',\n"," 'city',\n"," \"''\",\n"," '.']"]},"metadata":{"tags":[]},"execution_count":40}]},{"cell_type":"code","metadata":{"id":"tAHBTIEVLLnS","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1571240385207,"user_tz":-60,"elapsed":3401,"user":{"displayName":"Dipanjana Colab","photoUrl":"","userId":"10332801813381954113"}},"outputId":"17e11ff6-fc3e-47cf-aea6-edd52f914b67"},"source":["# Evaluation\n","print(regexp_tagger.tag(word_tokenize('bob made a book collector happy the other day')))\n","print(regexp_tagger.evaluate(brown_tagged_sents))\n","\n","# O/P: Very unsatisfactory"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[('bob', 'NN'), ('made', 'NN'), ('a', 'AT'), ('book', 'NN'), ('collector', 'NN'), ('happy', 'NN'), ('the', 'AT'), ('other', 'NN'), ('day', 'NN')]\n","0.4461085585854367\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8zeoxLJQLMK3"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9UKHlP2YMtwt"},"source":["**Combining N-Gram Tagging**"]},{"cell_type":"code","metadata":{"id":"czl80SkSLMqy"},"source":["# Creating train and test set\n","size = int(len(brown_tagged_sents) * 0.9)\n","train_sents = brown_tagged_sents[:size]\n","test_sents = brown_tagged_sents[size:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MG3pYA-8LNQr","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1571241503756,"user_tz":-60,"elapsed":3405,"user":{"displayName":"Dipanjana Colab","photoUrl":"","userId":"10332801813381954113"}},"outputId":"b79ca42d-d9d2-46b8-81ba-f87c00497820"},"source":["t0 = nltk.DefaultTagger('NN')\n","t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n","t2 = nltk.BigramTagger(train_sents, backoff=t1)\n","t3 = nltk.TrigramTagger(train_sents, backoff=t2)\n","\n","print (\"\\nEvaluation mix default/1G/2G/3G on train set \", t3.evaluate(train_sents))\n","print (\"Evaluation mix default/1G/2G/3G on test set \", t3.evaluate(test_sents))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Evaluation mix default/1G/2G/3G on train set  0.9829321372941086\n","Evaluation mix default/1G/2G/3G on test set  0.843317053722715\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"k0ruLoUTLN1N","colab":{"base_uri":"https://localhost:8080/","height":170},"executionInfo":{"status":"ok","timestamp":1571241588698,"user_tz":-60,"elapsed":341,"user":{"displayName":"Dipanjana Colab","photoUrl":"","userId":"10332801813381954113"}},"outputId":"73642e58-6b24-40bc-e231-a487383f80fa"},"source":["t3.tag(word_tokenize('bob made a book collector happy the other day'))\n","\n","# O/P: Wrong\n","# # 'book' should be adjective but tagged as 'NN' \n","# 'happy' should be adverb but tagged as 'AP' "],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('bob', 'NN'),\n"," ('made', 'VBN'),\n"," ('a', 'AT'),\n"," ('book', 'NN'),\n"," ('collector', 'NN'),\n"," ('happy', 'JJ'),\n"," ('the', 'AT'),\n"," ('other', 'AP'),\n"," ('day', 'NN')]"]},"metadata":{"tags":[]},"execution_count":48}]},{"cell_type":"code","metadata":{"id":"i5L6ZBk9TB1q"},"source":[""],"execution_count":null,"outputs":[]}]}