{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Sentiment_Analysis.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"AGkiYoL2qvg8","colab_type":"code","colab":{}},"source":["import nltk\n","from nltk import word_tokenize\n","from nltk import download\n","from nltk.corpus import stopwords\n","import numpy as np\n","import pandas as pd\n","import re"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HGBHe4a_rC58","colab_type":"code","outputId":"9b706146-513f-4bcd-d9a9-b884143030a8","executionInfo":{"status":"ok","timestamp":1578059042765,"user_tz":0,"elapsed":2080,"user":{"displayName":"Dipanjana Colab","photoUrl":"","userId":"10332801813381954113"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["nltk.download('all')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading collection 'all'\n","[nltk_data]    | \n","[nltk_data]    | Downloading package abc to /root/nltk_data...\n","[nltk_data]    |   Package abc is already up-to-date!\n","[nltk_data]    | Downloading package alpino to /root/nltk_data...\n","[nltk_data]    |   Package alpino is already up-to-date!\n","[nltk_data]    | Downloading package biocreative_ppi to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n","[nltk_data]    | Downloading package brown to /root/nltk_data...\n","[nltk_data]    |   Package brown is already up-to-date!\n","[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n","[nltk_data]    |   Package brown_tei is already up-to-date!\n","[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n","[nltk_data]    |   Package cess_cat is already up-to-date!\n","[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n","[nltk_data]    |   Package cess_esp is already up-to-date!\n","[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n","[nltk_data]    |   Package chat80 is already up-to-date!\n","[nltk_data]    | Downloading package city_database to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package city_database is already up-to-date!\n","[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n","[nltk_data]    |   Package cmudict is already up-to-date!\n","[nltk_data]    | Downloading package comparative_sentences to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package comparative_sentences is already up-to-\n","[nltk_data]    |       date!\n","[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n","[nltk_data]    |   Package comtrans is already up-to-date!\n","[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n","[nltk_data]    |   Package conll2000 is already up-to-date!\n","[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n","[nltk_data]    |   Package conll2002 is already up-to-date!\n","[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n","[nltk_data]    |   Package conll2007 is already up-to-date!\n","[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n","[nltk_data]    |   Package crubadan is already up-to-date!\n","[nltk_data]    | Downloading package dependency_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package dependency_treebank is already up-to-date!\n","[nltk_data]    | Downloading package dolch to /root/nltk_data...\n","[nltk_data]    |   Package dolch is already up-to-date!\n","[nltk_data]    | Downloading package europarl_raw to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package europarl_raw is already up-to-date!\n","[nltk_data]    | Downloading package floresta to /root/nltk_data...\n","[nltk_data]    |   Package floresta is already up-to-date!\n","[nltk_data]    | Downloading package framenet_v15 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package framenet_v15 is already up-to-date!\n","[nltk_data]    | Downloading package framenet_v17 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package framenet_v17 is already up-to-date!\n","[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n","[nltk_data]    |   Package gazetteers is already up-to-date!\n","[nltk_data]    | Downloading package genesis to /root/nltk_data...\n","[nltk_data]    |   Package genesis is already up-to-date!\n","[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]    |   Package gutenberg is already up-to-date!\n","[nltk_data]    | Downloading package ieer to /root/nltk_data...\n","[nltk_data]    |   Package ieer is already up-to-date!\n","[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n","[nltk_data]    |   Package inaugural is already up-to-date!\n","[nltk_data]    | Downloading package indian to /root/nltk_data...\n","[nltk_data]    |   Package indian is already up-to-date!\n","[nltk_data]    | Downloading package jeita to /root/nltk_data...\n","[nltk_data]    |   Package jeita is already up-to-date!\n","[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n","[nltk_data]    |   Package kimmo is already up-to-date!\n","[nltk_data]    | Downloading package knbc to /root/nltk_data...\n","[nltk_data]    |   Package knbc is already up-to-date!\n","[nltk_data]    | Downloading package lin_thesaurus to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n","[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n","[nltk_data]    |   Package mac_morpho is already up-to-date!\n","[nltk_data]    | Downloading package machado to /root/nltk_data...\n","[nltk_data]    |   Package machado is already up-to-date!\n","[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n","[nltk_data]    |   Package masc_tagged is already up-to-date!\n","[nltk_data]    | Downloading package moses_sample to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package moses_sample is already up-to-date!\n","[nltk_data]    | Downloading package movie_reviews to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package movie_reviews is already up-to-date!\n","[nltk_data]    | Downloading package names to /root/nltk_data...\n","[nltk_data]    |   Package names is already up-to-date!\n","[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n","[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n","[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n","[nltk_data]    |   Package nps_chat is already up-to-date!\n","[nltk_data]    | Downloading package omw to /root/nltk_data...\n","[nltk_data]    |   Package omw is already up-to-date!\n","[nltk_data]    | Downloading package opinion_lexicon to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n","[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n","[nltk_data]    |   Package paradigms is already up-to-date!\n","[nltk_data]    | Downloading package pil to /root/nltk_data...\n","[nltk_data]    |   Package pil is already up-to-date!\n","[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n","[nltk_data]    |   Package pl196x is already up-to-date!\n","[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n","[nltk_data]    |   Package ppattach is already up-to-date!\n","[nltk_data]    | Downloading package problem_reports to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package problem_reports is already up-to-date!\n","[nltk_data]    | Downloading package propbank to /root/nltk_data...\n","[nltk_data]    |   Package propbank is already up-to-date!\n","[nltk_data]    | Downloading package ptb to /root/nltk_data...\n","[nltk_data]    |   Package ptb is already up-to-date!\n","[nltk_data]    | Downloading package product_reviews_1 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n","[nltk_data]    | Downloading package product_reviews_2 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n","[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n","[nltk_data]    |   Package pros_cons is already up-to-date!\n","[nltk_data]    | Downloading package qc to /root/nltk_data...\n","[nltk_data]    |   Package qc is already up-to-date!\n","[nltk_data]    | Downloading package reuters to /root/nltk_data...\n","[nltk_data]    |   Package reuters is already up-to-date!\n","[nltk_data]    | Downloading package rte to /root/nltk_data...\n","[nltk_data]    |   Package rte is already up-to-date!\n","[nltk_data]    | Downloading package semcor to /root/nltk_data...\n","[nltk_data]    |   Package semcor is already up-to-date!\n","[nltk_data]    | Downloading package senseval to /root/nltk_data...\n","[nltk_data]    |   Package senseval is already up-to-date!\n","[nltk_data]    | Downloading package sentiwordnet to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package sentiwordnet is already up-to-date!\n","[nltk_data]    | Downloading package sentence_polarity to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package sentence_polarity is already up-to-date!\n","[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n","[nltk_data]    |   Package shakespeare is already up-to-date!\n","[nltk_data]    | Downloading package sinica_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package sinica_treebank is already up-to-date!\n","[nltk_data]    | Downloading package smultron to /root/nltk_data...\n","[nltk_data]    |   Package smultron is already up-to-date!\n","[nltk_data]    | Downloading package state_union to /root/nltk_data...\n","[nltk_data]    |   Package state_union is already up-to-date!\n","[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n","[nltk_data]    |   Package stopwords is already up-to-date!\n","[nltk_data]    | Downloading package subjectivity to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package subjectivity is already up-to-date!\n","[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n","[nltk_data]    |   Package swadesh is already up-to-date!\n","[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n","[nltk_data]    |   Package switchboard is already up-to-date!\n","[nltk_data]    | Downloading package timit to /root/nltk_data...\n","[nltk_data]    |   Package timit is already up-to-date!\n","[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n","[nltk_data]    |   Package toolbox is already up-to-date!\n","[nltk_data]    | Downloading package treebank to /root/nltk_data...\n","[nltk_data]    |   Package treebank is already up-to-date!\n","[nltk_data]    | Downloading package twitter_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package twitter_samples is already up-to-date!\n","[nltk_data]    | Downloading package udhr to /root/nltk_data...\n","[nltk_data]    |   Package udhr is already up-to-date!\n","[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n","[nltk_data]    |   Package udhr2 is already up-to-date!\n","[nltk_data]    | Downloading package unicode_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package unicode_samples is already up-to-date!\n","[nltk_data]    | Downloading package universal_treebanks_v20 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n","[nltk_data]    |       date!\n","[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n","[nltk_data]    |   Package verbnet is already up-to-date!\n","[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n","[nltk_data]    |   Package verbnet3 is already up-to-date!\n","[nltk_data]    | Downloading package webtext to /root/nltk_data...\n","[nltk_data]    |   Package webtext is already up-to-date!\n","[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n","[nltk_data]    |   Package wordnet is already up-to-date!\n","[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n","[nltk_data]    |   Package wordnet_ic is already up-to-date!\n","[nltk_data]    | Downloading package words to /root/nltk_data...\n","[nltk_data]    |   Package words is already up-to-date!\n","[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n","[nltk_data]    |   Package ycoe is already up-to-date!\n","[nltk_data]    | Downloading package rslp to /root/nltk_data...\n","[nltk_data]    |   Package rslp is already up-to-date!\n","[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n","[nltk_data]    |       to-date!\n","[nltk_data]    | Downloading package universal_tagset to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package universal_tagset is already up-to-date!\n","[nltk_data]    | Downloading package maxent_ne_chunker to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n","[nltk_data]    | Downloading package punkt to /root/nltk_data...\n","[nltk_data]    |   Package punkt is already up-to-date!\n","[nltk_data]    | Downloading package book_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package book_grammars is already up-to-date!\n","[nltk_data]    | Downloading package sample_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package sample_grammars is already up-to-date!\n","[nltk_data]    | Downloading package spanish_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package spanish_grammars is already up-to-date!\n","[nltk_data]    | Downloading package basque_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package basque_grammars is already up-to-date!\n","[nltk_data]    | Downloading package large_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package large_grammars is already up-to-date!\n","[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n","[nltk_data]    |   Package tagsets is already up-to-date!\n","[nltk_data]    | Downloading package snowball_data to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package snowball_data is already up-to-date!\n","[nltk_data]    | Downloading package bllip_wsj_no_aux to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n","[nltk_data]    | Downloading package word2vec_sample to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package word2vec_sample is already up-to-date!\n","[nltk_data]    | Downloading package panlex_swadesh to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n","[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n","[nltk_data]    |   Package mte_teip5 is already up-to-date!\n","[nltk_data]    | Downloading package averaged_perceptron_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n","[nltk_data]    |       to-date!\n","[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n","[nltk_data]    |       up-to-date!\n","[nltk_data]    | Downloading package perluniprops to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package perluniprops is already up-to-date!\n","[nltk_data]    | Downloading package nonbreaking_prefixes to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n","[nltk_data]    | Downloading package vader_lexicon to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package vader_lexicon is already up-to-date!\n","[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n","[nltk_data]    |   Package porter_test is already up-to-date!\n","[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n","[nltk_data]    |   Package wmt15_eval is already up-to-date!\n","[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n","[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n","[nltk_data]    | \n","[nltk_data]  Done downloading collection all\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"_9mh2mbyYXHu","colab_type":"code","colab":{}},"source":["# read csv into a dataframe\n","train_data = pd.read_csv(\"/train_tweets.csv\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JdPNwWpYYXDc","colab_type":"code","outputId":"89015c02-9f67-4524-8636-89189d966efe","executionInfo":{"status":"ok","timestamp":1578059049273,"user_tz":0,"elapsed":365,"user":{"displayName":"Dipanjana Colab","photoUrl":"","userId":"10332801813381954113"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["train_data.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>label</th>\n","      <th>tweet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>@user when a father is dysfunctional and is s...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>@user @user thanks for #lyft credit i can't us...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>bihday your majesty</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>#model   i love u take with u all the time in ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>factsguide: society now    #motivation</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id  label                                              tweet\n","0   1      0   @user when a father is dysfunctional and is s...\n","1   2      0  @user @user thanks for #lyft credit i can't us...\n","2   3      0                                bihday your majesty\n","3   4      0  #model   i love u take with u all the time in ...\n","4   5      0             factsguide: society now    #motivation"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"id":"CX7vFcanYW9s","colab_type":"code","outputId":"bfb14048-0c64-4650-b884-c65dd1519a03","executionInfo":{"status":"ok","timestamp":1578059050376,"user_tz":0,"elapsed":376,"user":{"displayName":"Dipanjana Colab","photoUrl":"","userId":"10332801813381954113"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"source":["# exploratory data analysis\n","train_data.info()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 31962 entries, 0 to 31961\n","Data columns (total 3 columns):\n","id       31962 non-null int64\n","label    31962 non-null int64\n","tweet    31962 non-null object\n","dtypes: int64(2), object(1)\n","memory usage: 749.2+ KB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Cl3mljVNYW5e","colab_type":"code","colab":{}},"source":["# data clean up activities\n","\n","# function to remove unwanted columns from dataframe\n","def drop_features(features,data):\n","    data.drop(features,inplace=True,axis=1)\n","\n","# cleaning up the 'tweet' \n","def process_tweet(tweet):\n","    return \" \".join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])\", \"\",tweet.lower()).split())\n","\n","# adding a new column('processed_tweets') containing cleaned up tweets\n","train_data['processed_tweets'] = train_data['tweet'].apply(process_tweet)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IP5jAD6rpprd","colab_type":"code","outputId":"dbc6f4d4-7dbc-4a1f-a091-07ae5cf67ac0","executionInfo":{"status":"ok","timestamp":1578059081023,"user_tz":0,"elapsed":378,"user":{"displayName":"Dipanjana Colab","photoUrl":"","userId":"10332801813381954113"}},"colab":{"base_uri":"https://localhost:8080/","height":359}},"source":["train_data.head(10)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>label</th>\n","      <th>tweet</th>\n","      <th>processed_tweets</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>@user when a father is dysfunctional and is s...</td>\n","      <td>when a father is dysfunctional and is so selfi...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>@user @user thanks for #lyft credit i can't us...</td>\n","      <td>thanks for lyft credit i cant use cause they d...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>bihday your majesty</td>\n","      <td>bihday your majesty</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>#model   i love u take with u all the time in ...</td>\n","      <td>model i love u take with u all the time in ur</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>factsguide: society now    #motivation</td>\n","      <td>factsguide society now motivation</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>[2/2] huge fan fare and big talking before the...</td>\n","      <td>22 huge fan fare and big talking before they l...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>7</td>\n","      <td>0</td>\n","      <td>@user camping tomorrow @user @user @user @use...</td>\n","      <td>camping tomorrow danny</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>8</td>\n","      <td>0</td>\n","      <td>the next school year is the year for exams.ð...</td>\n","      <td>the next school year is the year for exams can...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>9</td>\n","      <td>0</td>\n","      <td>we won!!! love the land!!! #allin #cavs #champ...</td>\n","      <td>we won love the land allin cavs champions clev...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>10</td>\n","      <td>0</td>\n","      <td>@user @user welcome here !  i'm   it's so #gr...</td>\n","      <td>welcome here im its so gr8</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id  ...                                   processed_tweets\n","0   1  ...  when a father is dysfunctional and is so selfi...\n","1   2  ...  thanks for lyft credit i cant use cause they d...\n","2   3  ...                                bihday your majesty\n","3   4  ...      model i love u take with u all the time in ur\n","4   5  ...                  factsguide society now motivation\n","5   6  ...  22 huge fan fare and big talking before they l...\n","6   7  ...                             camping tomorrow danny\n","7   8  ...  the next school year is the year for exams can...\n","8   9  ...  we won love the land allin cavs champions clev...\n","9  10  ...                         welcome here im its so gr8\n","\n","[10 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"YYXxKDqQqDAp","colab_type":"code","colab":{}},"source":["drop_features(['id','tweet'],train_data)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sVgYPUE6qC8v","colab_type":"code","outputId":"8073a8c6-0683-4ffe-ffc0-42aaf01d1d20","executionInfo":{"status":"ok","timestamp":1578059209533,"user_tz":0,"elapsed":381,"user":{"displayName":"Dipanjana Colab","photoUrl":"","userId":"10332801813381954113"}},"colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["train_data.info()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 31962 entries, 0 to 31961\n","Data columns (total 2 columns):\n","label               31962 non-null int64\n","processed_tweets    31962 non-null object\n","dtypes: int64(1), object(1)\n","memory usage: 499.5+ KB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NQb0h7Y6YWyd","colab_type":"code","outputId":"83b4babc-869c-4d83-9b8a-0c560d59f1a7","executionInfo":{"status":"ok","timestamp":1578065788340,"user_tz":0,"elapsed":455,"user":{"displayName":"Dipanjana Colab","photoUrl":"","userId":"10332801813381954113"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# separating out the labels for facilitating training process\n","# and splitting dataset into training and test sets\n","\n","from sklearn.model_selection import train_test_split\n","\n","x_train, x_test, y_train, y_test = train_test_split(train_data[\"processed_tweets\"],train_data[\"label\"], test_size = 0.2)\n"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'happy fathers day'"]},"metadata":{"tags":[]},"execution_count":57}]},{"cell_type":"code","metadata":{"id":"4aRscmzxCI7n","colab_type":"code","colab":{}},"source":["# vectorisation of training data using tfidf vectoriser\n","\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n","\n","count_vect = CountVectorizer(stop_words='english')\n","transformer = TfidfTransformer(norm='l2',sublinear_tf=True)\n","\n","x_train_counts = count_vect.fit_transform(x_train)\n","x_train_tfidf = transformer.fit_transform(x_train_counts)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y01bauyQCFl4","colab_type":"code","outputId":"44e6f51a-5682-4745-c0f8-7792da1339a8","executionInfo":{"status":"ok","timestamp":1578059375534,"user_tz":0,"elapsed":379,"user":{"displayName":"Dipanjana Colab","photoUrl":"","userId":"10332801813381954113"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["print(x_train_counts.shape)\n","print(x_train_tfidf.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(25569, 35467)\n","(25569, 35467)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eaWwzoUxqyFs","colab_type":"code","colab":{}},"source":["# vectorisation of test data using tfidf vectoriser\n","\n","x_test_counts = count_vect.transform(x_test)\n","x_test_tfidf = transformer.transform(x_test_counts)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4mZDWbZ9rHao","colab_type":"code","outputId":"166992c5-f405-4cc1-b0b8-36f945618ab2","executionInfo":{"status":"ok","timestamp":1578059486037,"user_tz":0,"elapsed":381,"user":{"displayName":"Dipanjana Colab","photoUrl":"","userId":"10332801813381954113"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["print(x_test_counts.shape)\n","print(x_test_tfidf.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(6393, 35467)\n","(6393, 35467)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"f27qZUpdsAcu","colab_type":"text"},"source":["# Training the model"]},{"cell_type":"markdown","metadata":{"id":"GRrlOiK59-xz","colab_type":"text"},"source":["# RandomForestClassifier"]},{"cell_type":"code","metadata":{"id":"rNnrX26arNEv","colab_type":"code","outputId":"747abd8e-8b62-40b5-fa40-c86788a98c6d","executionInfo":{"status":"ok","timestamp":1578059884340,"user_tz":0,"elapsed":37063,"user":{"displayName":"Dipanjana Colab","photoUrl":"","userId":"10332801813381954113"}},"colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["from sklearn.ensemble import RandomForestClassifier\n","model = RandomForestClassifier(n_estimators=200)\n","model.fit(x_train_tfidf,y_train)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n","                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n","                       min_impurity_decrease=0.0, min_impurity_split=None,\n","                       min_samples_leaf=1, min_samples_split=2,\n","                       min_weight_fraction_leaf=0.0, n_estimators=200,\n","                       n_jobs=None, oob_score=False, random_state=None,\n","                       verbose=0, warm_start=False)"]},"metadata":{"tags":[]},"execution_count":40}]},{"cell_type":"code","metadata":{"id":"pAQAdUZAslUI","colab_type":"code","outputId":"248f0753-b527-4836-a5b3-44342dd83811","executionInfo":{"status":"ok","timestamp":1578060046604,"user_tz":0,"elapsed":3147,"user":{"displayName":"Dipanjana Colab","photoUrl":"","userId":"10332801813381954113"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["predictions = model.predict(x_test_tfidf)\n","print(predictions)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[0 0 0 ... 0 0 0]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rm0Z0fq7tVQG","colab_type":"code","outputId":"0ac0d817-a083-4f27-d880-522588ad453e","executionInfo":{"status":"ok","timestamp":1578060922255,"user_tz":0,"elapsed":391,"user":{"displayName":"Dipanjana Colab","photoUrl":"","userId":"10332801813381954113"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# model accuracy\n","\n","from sklearn.metrics import accuracy_score\n","print(accuracy_score(y_test,predictions))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0.9607383075238543\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2czFFQFcwrrK","colab_type":"code","colab":{}},"source":["# testing new set of data on the trained model\n","\n","test_data = pd.read_csv(\"/content/drive/My Drive/NLP/Sentiment_Analysis/test_tweets.csv\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oUgY_Sfp0VeF","colab_type":"code","outputId":"a8f48a70-74d6-4cf8-e2f7-6c0d8558e2b1","executionInfo":{"status":"ok","timestamp":1578062061889,"user_tz":0,"elapsed":1051,"user":{"displayName":"Dipanjana Colab","photoUrl":"","userId":"10332801813381954113"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# data pre-processing of test dataset\n","\n","test_data['processed_tweet'] = test_data['tweet'].apply(process_tweet)\n","\n","drop_features(['tweet'],test_data)\n","\n","train_counts = count_vect.fit_transform(train_data['processed_tweets'])\n","test_counts = count_vect.transform(test_data['processed_tweet'])\n","\n","print(train_counts.shape)\n","print(test_counts.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(31962, 41120)\n","(17197, 41120)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3y6kkR2W1BwU","colab_type":"code","colab":{}},"source":["train_tfidf = transformer.fit_transform(train_counts)\n","test_tfidf = transformer.transform(test_counts)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"M5xvmYQj1GAk","colab_type":"code","outputId":"60bb3b4c-10d4-4460-e052-237bd1041d73","executionInfo":{"status":"ok","timestamp":1578062143761,"user_tz":0,"elapsed":51100,"user":{"displayName":"Dipanjana Colab","photoUrl":"","userId":"10332801813381954113"}},"colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["model.fit(train_tfidf,train_data['label'])"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n","                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n","                       min_impurity_decrease=0.0, min_impurity_split=None,\n","                       min_samples_leaf=1, min_samples_split=2,\n","                       min_weight_fraction_leaf=0.0, n_estimators=200,\n","                       n_jobs=None, oob_score=False, random_state=None,\n","                       verbose=0, warm_start=False)"]},"metadata":{"tags":[]},"execution_count":46}]},{"cell_type":"code","metadata":{"id":"PPqJF_BA1JiZ","colab_type":"code","colab":{}},"source":["predictions = model.predict(test_tfidf)\n","\n","# putting the predictions into dataframe\n","final_result = pd.DataFrame({'id':test_data['id'],'label':predictions})"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mUK71RbD1bY9","colab_type":"code","outputId":"45b3d0c6-b617-463a-f851-84b838728456","executionInfo":{"status":"ok","timestamp":1578062206151,"user_tz":0,"elapsed":414,"user":{"displayName":"Dipanjana Colab","photoUrl":"","userId":"10332801813381954113"}},"colab":{"base_uri":"https://localhost:8080/","height":419}},"source":["final_result"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>31963</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>31964</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>31965</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>31966</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>31967</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>17192</th>\n","      <td>49155</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>17193</th>\n","      <td>49156</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>17194</th>\n","      <td>49157</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>17195</th>\n","      <td>49158</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>17196</th>\n","      <td>49159</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>17197 rows × 2 columns</p>\n","</div>"],"text/plain":["          id  label\n","0      31963      0\n","1      31964      1\n","2      31965      0\n","3      31966      0\n","4      31967      0\n","...      ...    ...\n","17192  49155      1\n","17193  49156      0\n","17194  49157      0\n","17195  49158      0\n","17196  49159      0\n","\n","[17197 rows x 2 columns]"]},"metadata":{"tags":[]},"execution_count":49}]},{"cell_type":"markdown","metadata":{"id":"kcEmvgQw-E3p","colab_type":"text"},"source":["# Deep Learning"]},{"cell_type":"code","metadata":{"id":"iHDgpUGoBCnP","colab_type":"code","colab":{}},"source":["from numpy import array\n","from keras.preprocessing.text import one_hot\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers.core import Activation, Dropout, Dense\n","from keras.layers import Conv2D, Conv1D, LSTM, Bidirectional, GlobalMaxPooling1D, Flatten\n","from keras.layers.embeddings import Embedding\n","from sklearn.model_selection import train_test_split\n","from keras.preprocessing.text import Tokenizer"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J56oaD6ODm19","colab_type":"code","colab":{}},"source":["# Train -test splitting of teh given dataset\n","\n","X_train, X_test, y_train, y_test = train_test_split(train_data[\"processed_tweets\"],train_data[\"label\"], test_size=0.20)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rlhTmR-tEPRz","colab_type":"text"},"source":["The Tokenizer class from the keras.preprocessing.text module to create a word-to-index dictionary. In the word-to-index dictionary, each word in the corpus is used as a key, while a corresponding unique index is used as the value for the key"]},{"cell_type":"code","metadata":{"id":"Mm9N47in1lJ8","colab_type":"code","colab":{}},"source":["# further preprocessing of the cleaned up texts before applying word-embedding\n","\n","tokenizer = Tokenizer(num_words=5000)\n","tokenizer.fit_on_texts(X_train)\n","\n","X_train = tokenizer.texts_to_sequences(X_train)\n","X_test = tokenizer.texts_to_sequences(X_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2x8XQlrpBudD","colab_type":"code","outputId":"e52d6431-c04e-4a2e-df8b-17523a865ac5","executionInfo":{"status":"ok","timestamp":1578066429646,"user_tz":0,"elapsed":387,"user":{"displayName":"Dipanjana Colab","photoUrl":"","userId":"10332801813381954113"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["# X_train and X_test are lists of sentences where each sentence is represented as a list of integers \n","#(each word in the corpus is used as a key, while a corresponding unique index is used as the value for the key thus a list of integers)\n","print(len(X_train))\n","\n","print(X_train[28])\n","\n","# total number of different words present\n","vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n","print(vocab_size)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["25569\n","[138, 67, 18]\n","35932\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"w221iJYbCywT","colab_type":"code","colab":{}},"source":["# setting up maximum length for any sentence \n","maxlen = 100\n","\n","# padding: adding 0 at the end of the list until it reaches the max length of 100 \n","X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n","X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UMzdr7itFyy2","colab_type":"code","colab":{}},"source":["# Using GloVe embeddings to create our feature matrix. \n","# load the GloVe word embeddings and create a dictionary that will contain words as keys and their corresponding embedding list as values\n","\n","from numpy import array\n","from numpy import asarray\n","from numpy import zeros\n","\n","embeddings_dictionary = dict()\n","glove_file = open('/glove.6B.100d.txt', encoding=\"utf8\")\n","\n","for line in glove_file:\n","    records = line.split()\n","    word = records[0]\n","    vector_dimensions = asarray(records[1:], dtype='float32')\n","    embeddings_dictionary [word] = vector_dimensions\n","glove_file.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-JzzSZUPHmf5","colab_type":"code","colab":{}},"source":["# creating an embedding matrix where each row number will correspond to the index of the word in the corpus\n","# the matrix will have 100 columns where each column will contain the GloVe word embeddings for the words in our corpus\n","\n","embedding_matrix = zeros((vocab_size, 100))\n","for word, index in tokenizer.word_index.items():\n","    embedding_vector = embeddings_dictionary.get(word)\n","    if embedding_vector is not None:\n","        embedding_matrix[index] = embedding_vector"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VTr0XCykH6tw","colab_type":"code","outputId":"19c272e5-f611-4044-c2ab-a8b463134550","executionInfo":{"status":"ok","timestamp":1578067982848,"user_tz":0,"elapsed":422,"user":{"displayName":"Dipanjana Colab","photoUrl":"","userId":"10332801813381954113"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["print(embedding_matrix.shape)\n","print(embedding_matrix.size)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(35932, 100)\n","3593200\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gYroTallKJh4","colab_type":"text"},"source":["# Classification with Simple Neural Network"]},{"cell_type":"code","metadata":{"id":"nxIMRD_yJ3W0","colab_type":"code","outputId":"61e4cbd8-8b49-4d0c-f894-4e9a52cadb26","executionInfo":{"status":"ok","timestamp":1578067985605,"user_tz":0,"elapsed":560,"user":{"displayName":"Dipanjana Colab","photoUrl":"","userId":"10332801813381954113"}},"colab":{"base_uri":"https://localhost:8080/","height":309}},"source":["model = Sequential()\n","\n","# Since we are not training our own embeddings and using the GloVe embedding, set trainable to False and in the weights attribute we pass our own embedding matrix\n","# set output length to 100( = input length of embedding layer)\n","embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n","\n","# The embedding layer is then added to the model\n","model.add(embedding_layer)\n","\n","# since we are directly connecting our embedding layer to densely connected layer, we flatten the embedding layer.\n","model.add(Flatten())\n","model.add(Dense(1, activation='sigmoid'))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZHlNKK4gLWVv","colab_type":"code","outputId":"fee3145c-e53b-4a4b-92f2-0f3acc8a3645","executionInfo":{"status":"ok","timestamp":1578068003947,"user_tz":0,"elapsed":444,"user":{"displayName":"Dipanjana Colab","photoUrl":"","userId":"10332801813381954113"}},"colab":{"base_uri":"https://localhost:8080/","height":411}},"source":["# model compilation\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n","\n","print(model.summary())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","Model: \"sequential_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_1 (Embedding)      (None, 100, 100)          3593200   \n","_________________________________________________________________\n","flatten_1 (Flatten)          (None, 10000)             0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 1)                 10001     \n","=================================================================\n","Total params: 3,603,201\n","Trainable params: 10,001\n","Non-trainable params: 3,593,200\n","_________________________________________________________________\n","None\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"X2KeJ1hCL6CZ","colab_type":"text"},"source":["**Explanation of the model compilation output:**\n","\n","Since there are 35932 words in our corpus and each word is represented as a 100-dimensional vector, the number of trainable parameter will be 35932x100 in the embedding layer. In the flattening layer, we simply multiply rows and column. Finally in the dense layer the number of parameters are 10000 (from the flattening layer) and 1 for the bias parameter, for a total of 10001."]},{"cell_type":"code","metadata":{"id":"jIfz0JOULsno","colab_type":"code","outputId":"5288451d-4138-4aae-ae96-52476df2c2d7","executionInfo":{"status":"ok","timestamp":1578068169760,"user_tz":0,"elapsed":4036,"user":{"displayName":"Dipanjana Colab","photoUrl":"","userId":"10332801813381954113"}},"colab":{"base_uri":"https://localhost:8080/","height":326}},"source":["# training the model\n","history = model.fit(X_train, y_train, batch_size=128, epochs=6, verbose=1, validation_split=0.2)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n","\n","Train on 20455 samples, validate on 5114 samples\n","Epoch 1/6\n","20455/20455 [==============================] - 1s 41us/step - loss: 0.2690 - acc: 0.9220 - val_loss: 0.2211 - val_acc: 0.9300\n","Epoch 2/6\n","20455/20455 [==============================] - 1s 25us/step - loss: 0.2010 - acc: 0.9357 - val_loss: 0.2000 - val_acc: 0.9366\n","Epoch 3/6\n","20455/20455 [==============================] - 0s 24us/step - loss: 0.1834 - acc: 0.9389 - val_loss: 0.1915 - val_acc: 0.9388\n","Epoch 4/6\n","20455/20455 [==============================] - 1s 25us/step - loss: 0.1728 - acc: 0.9421 - val_loss: 0.1854 - val_acc: 0.9390\n","Epoch 5/6\n","20455/20455 [==============================] - 1s 26us/step - loss: 0.1656 - acc: 0.9444 - val_loss: 0.1832 - val_acc: 0.9398\n","Epoch 6/6\n","20455/20455 [==============================] - 1s 26us/step - loss: 0.1593 - acc: 0.9462 - val_loss: 0.1800 - val_acc: 0.9417\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WxH356sMMULp","colab_type":"code","outputId":"8b5737dd-6476-48f5-a8a0-297138fa53c0","executionInfo":{"status":"ok","timestamp":1578068202146,"user_tz":0,"elapsed":586,"user":{"displayName":"Dipanjana Colab","photoUrl":"","userId":"10332801813381954113"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["# model evaluation:\n","score = model.evaluate(X_test, y_test, verbose=1)\n","print(\"Test Score:\", score[0])\n","print(\"Test Accuracy:\", score[1])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["6393/6393 [==============================] - 0s 34us/step\n","Test Score: 0.17698338600078062\n","Test Accuracy: 0.9421241982580257\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TSmldzy6ToYx","colab_type":"text"},"source":["**Observation**\n","\n","The model is overfitting as there is a vast difference between the training and test accuracy"]},{"cell_type":"markdown","metadata":{"id":"WEt9vb9OPQyG","colab_type":"text"},"source":["# Classification with a Convolutional Neural Network"]},{"cell_type":"code","metadata":{"id":"8l0Sx4lvMc94","colab_type":"code","colab":{}},"source":["model = Sequential()\n","\n","embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n","model.add(embedding_layer)\n","\n","#one-dimensional convolutional layer with 128 features, or kernels. The kernel size is 5\n","model.add(Conv1D(128, 5, activation='relu'))\n","\n","# add a global max pooling layer to reduce feature size\n","model.add(GlobalMaxPooling1D())\n","model.add(Dense(1, activation='sigmoid'))\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ja_-dmZCQ68Z","colab_type":"code","outputId":"84cd69dd-09e5-46fc-eb3c-5d3a2a60b196","executionInfo":{"status":"ok","timestamp":1578069755607,"user_tz":0,"elapsed":592,"user":{"displayName":"Dipanjana Colab","photoUrl":"","userId":"10332801813381954113"}},"colab":{"base_uri":"https://localhost:8080/","height":306}},"source":["print(model.summary())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Model: \"sequential_4\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_3 (Embedding)      (None, 100, 100)          3593200   \n","_________________________________________________________________\n","conv1d_1 (Conv1D)            (None, 96, 128)           64128     \n","_________________________________________________________________\n","global_max_pooling1d_1 (Glob (None, 128)               0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 1)                 129       \n","=================================================================\n","Total params: 3,657,457\n","Trainable params: 64,257\n","Non-trainable params: 3,593,200\n","_________________________________________________________________\n","None\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AqGWLuNOSmhU","colab_type":"text"},"source":["**Explanation of the model compilation output:**\n","In the above case we don't need to flatten our embedding layer.Also note: the feature size is now reduced using the pooling layer."]},{"cell_type":"code","metadata":{"id":"p2M71bYNSYOO","colab_type":"code","outputId":"cc8a8913-dab5-47a8-b081-819ab115613d","executionInfo":{"status":"ok","timestamp":1578069989876,"user_tz":0,"elapsed":76657,"user":{"displayName":"Dipanjana Colab","photoUrl":"","userId":"10332801813381954113"}},"colab":{"base_uri":"https://localhost:8080/","height":238}},"source":["# training the model:\n","history = model.fit(X_train, y_train, batch_size=128, epochs=6, verbose=1, validation_split=0.2)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 20455 samples, validate on 5114 samples\n","Epoch 1/6\n","20455/20455 [==============================] - 13s 637us/step - loss: 0.2373 - acc: 0.9236 - val_loss: 0.1712 - val_acc: 0.9437\n","Epoch 2/6\n","20455/20455 [==============================] - 13s 618us/step - loss: 0.1535 - acc: 0.9466 - val_loss: 0.1510 - val_acc: 0.9499\n","Epoch 3/6\n","20455/20455 [==============================] - 13s 614us/step - loss: 0.1280 - acc: 0.9553 - val_loss: 0.1396 - val_acc: 0.9552\n","Epoch 4/6\n","20455/20455 [==============================] - 12s 611us/step - loss: 0.1067 - acc: 0.9622 - val_loss: 0.1386 - val_acc: 0.9539\n","Epoch 5/6\n","20455/20455 [==============================] - 13s 611us/step - loss: 0.0872 - acc: 0.9718 - val_loss: 0.1310 - val_acc: 0.9603\n","Epoch 6/6\n","20455/20455 [==============================] - 13s 622us/step - loss: 0.0690 - acc: 0.9792 - val_loss: 0.1266 - val_acc: 0.9619\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"phxKXcC0S-2n","colab_type":"code","outputId":"06ec5440-cd30-4089-f2a0-1d49f6c2268d","executionInfo":{"status":"ok","timestamp":1578070016416,"user_tz":0,"elapsed":1760,"user":{"displayName":"Dipanjana Colab","photoUrl":"","userId":"10332801813381954113"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["# model evaluation:\n","\n","score = model.evaluate(X_test, y_test, verbose=1)\n","print(\"Test Score:\", score[0])\n","print(\"Test Accuracy:\", score[1])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["6393/6393 [==============================] - 1s 231us/step\n","Test Score: 0.1336600723078082\n","Test Accuracy: 0.9571406224720097\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zAAozJNjTZuu","colab_type":"text"},"source":["**Observation**\n","\n","CNN model is still overfitting as there is a vast difference between the training and test accuracy"]},{"cell_type":"markdown","metadata":{"id":"UDQBfbO7VJpQ","colab_type":"text"},"source":["# Classification with Recurrent Neural Network (LSTM)"]},{"cell_type":"code","metadata":{"id":"6n2Jq4OWTXnm","colab_type":"code","outputId":"1ace126b-48a8-4f6a-dfc6-f7d8a62aa035","executionInfo":{"status":"ok","timestamp":1578072393226,"user_tz":0,"elapsed":1457,"user":{"displayName":"Dipanjana Colab","photoUrl":"","userId":"10332801813381954113"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["model = Sequential()\n","embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n","model.add(embedding_layer)\n","\n","# create an bidirectional LSTM layer with 128 neurons (number of nueurons can be adjusted to have better model performance)\n","model.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2)))\n","\n","model.add(Dense(512, activation='relu'))\n","model.add(Dropout(0.50))\n","\n","model.add(Dense(1, activation='sigmoid'))\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GYCH7JFiVgml","colab_type":"code","outputId":"fe20d765-012c-41f6-beea-43c268b7872f","executionInfo":{"status":"ok","timestamp":1578072402167,"user_tz":0,"elapsed":826,"user":{"displayName":"Dipanjana Colab","photoUrl":"","userId":"10332801813381954113"}},"colab":{"base_uri":"https://localhost:8080/","height":340}},"source":["print(model.summary())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Model: \"sequential_8\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_7 (Embedding)      (None, 100, 100)          3593200   \n","_________________________________________________________________\n","bidirectional_1 (Bidirection (None, 256)               234496    \n","_________________________________________________________________\n","dense_4 (Dense)              (None, 512)               131584    \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 512)               0         \n","_________________________________________________________________\n","dense_5 (Dense)              (None, 1)                 513       \n","=================================================================\n","Total params: 3,959,793\n","Trainable params: 366,593\n","Non-trainable params: 3,593,200\n","_________________________________________________________________\n","None\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VdfUsxqVV_Og","colab_type":"code","outputId":"b1463ce3-613a-4bdc-87fb-87e218580d57","executionInfo":{"status":"ok","timestamp":1578073036306,"user_tz":0,"elapsed":626792,"user":{"displayName":"Dipanjana Colab","photoUrl":"","userId":"10332801813381954113"}},"colab":{"base_uri":"https://localhost:8080/","height":238}},"source":["# training the model\n","history = model.fit(X_train, y_train, batch_size=128, epochs=6, verbose=1, validation_split=0.2)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 20455 samples, validate on 5114 samples\n","Epoch 1/6\n","20455/20455 [==============================] - 106s 5ms/step - loss: 0.2226 - acc: 0.9314 - val_loss: 0.1911 - val_acc: 0.9363\n","Epoch 2/6\n","20455/20455 [==============================] - 104s 5ms/step - loss: 0.1735 - acc: 0.9391 - val_loss: 0.1582 - val_acc: 0.9460\n","Epoch 3/6\n","20455/20455 [==============================] - 104s 5ms/step - loss: 0.1578 - acc: 0.9450 - val_loss: 0.1458 - val_acc: 0.9505\n","Epoch 4/6\n","20455/20455 [==============================] - 103s 5ms/step - loss: 0.1506 - acc: 0.9480 - val_loss: 0.1490 - val_acc: 0.9505\n","Epoch 5/6\n","20455/20455 [==============================] - 103s 5ms/step - loss: 0.1397 - acc: 0.9519 - val_loss: 0.1343 - val_acc: 0.9535\n","Epoch 6/6\n","20455/20455 [==============================] - 104s 5ms/step - loss: 0.1304 - acc: 0.9540 - val_loss: 0.1330 - val_acc: 0.9519\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JPAkd52fWJCO","colab_type":"code","outputId":"cd54fceb-67e5-44c8-d583-7d81d03b2738","executionInfo":{"status":"ok","timestamp":1578071030255,"user_tz":0,"elapsed":5608,"user":{"displayName":"Dipanjana Colab","photoUrl":"","userId":"10332801813381954113"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["# model evaluation\n","\n","score = model.evaluate(X_test, y_test, verbose=1)\n","\n","print(\"Test Score:\", score[0])\n","print(\"Test Accuracy:\", score[1])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["6393/6393 [==============================] - 5s 827us/step\n","Test Score: 0.25460220199272365\n","Test Accuracy: 0.9297669324986013\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"I_XHZicgXoUm","colab_type":"code","outputId":"1f5d722f-94ae-4eb9-bad8-1d4b79e91d0a","executionInfo":{"status":"ok","timestamp":1578071425710,"user_tz":0,"elapsed":399,"user":{"displayName":"Dipanjana Colab","photoUrl":"","userId":"10332801813381954113"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["instance = train_data['processed_tweets'][1]\n","instance"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'thanks for lyft credit i cant use cause they dont offer wheelchair vans in pdx disapointed getthanked'"]},"metadata":{"tags":[]},"execution_count":104}]},{"cell_type":"code","metadata":{"id":"mC0qj1MbXOMg","colab_type":"code","outputId":"5047abec-48d6-41ed-ef95-50eae9bdde9f","executionInfo":{"status":"ok","timestamp":1578071433116,"user_tz":0,"elapsed":783,"user":{"displayName":"Dipanjana Colab","photoUrl":"","userId":"10332801813381954113"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# predicting any random tweet\n","\n","instance = tokenizer.texts_to_sequences(instance)\n","\n","flat_list = []\n","for sublist in instance:\n","    #print(sublist)\n","    for item in sublist:\n","        #print(item)\n","        flat_list.append(item)\n","\n","flat_list = [flat_list]\n","\n","instance = pad_sequences(flat_list, padding='post', maxlen=maxlen)\n","\n","model.predict(instance)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.06360893]], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":105}]},{"cell_type":"markdown","metadata":{"id":"kqe0oN34Y6OU","colab_type":"text"},"source":["As the score(0.06360893) is less than 0.5, the binary outcome of the result is considered to be 0 i.e it's not a hateful tweet"]},{"cell_type":"code","metadata":{"id":"1buvXSXTZhiU","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}