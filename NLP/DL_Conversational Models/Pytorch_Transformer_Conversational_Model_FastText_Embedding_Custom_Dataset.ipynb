{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"TPU","colab":{"name":"Pytorch_Transformer_FastText_Embedding_for_Custom_Conversational_Dataset.ipynb","provenance":[],"collapsed_sections":["eJokhyqrJjUj","skaVBc1UJKXv","zUAl3tEAJWIA"],"authorship_tag":"ABX9TyNLvwqK5kbIe0YEOsOfNALv"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jv1pzuPf77de","executionInfo":{"status":"ok","timestamp":1623151680212,"user_tz":-330,"elapsed":36687,"user":{"displayName":"ninja turtle","photoUrl":"","userId":"08410201356610850962"}},"outputId":"bfc9bc3d-5579-4a58-fa66-3f0ead773047"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d15jBY_B8f4M","executionInfo":{"status":"ok","timestamp":1623151688681,"user_tz":-330,"elapsed":4701,"user":{"displayName":"ninja turtle","photoUrl":"","userId":"08410201356610850962"}},"outputId":"fa928329-7308-48d5-ad1d-8e8cf7b53125"},"source":["!pip install Unidecode"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting Unidecode\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/25/723487ca2a52ebcee88a34d7d1f5a4b80b793f179ee0f62d5371938dfa01/Unidecode-1.2.0-py2.py3-none-any.whl (241kB)\n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 245kB 3.2MB/s \n","\u001b[?25hInstalling collected packages: Unidecode\n","Successfully installed Unidecode-1.2.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wJ1nAQCR8zNY"},"source":["# Imports\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","\n","import spacy\n","import numpy as np\n","import pandas as pd\n","\n","import random\n","import re\n","import math\n","import time\n","\n","import spacy\n","import torchtext\n","from torchtext.legacy.data import Field,BucketIterator,TabularDataset\n","from sklearn.model_selection import train_test_split\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZOqi-lqJ89ce"},"source":["torch.backends.cudnn.deterministic = True"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eJokhyqrJjUj"},"source":["# Data Preprocessing"]},{"cell_type":"code","metadata":{"id":"j8fVBNcX9O2k"},"source":["# replacing contraction words\n","\n","contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\",'i\\'m':'i am', \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", 'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization'}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lj124_Vp9TCK"},"source":["# dealing with spl characters\n","punct = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';',  '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', 'â€¢',  '~', '@', 'Â£', \n"," 'Â·', '_', '{', '}', 'Â©', '^', 'Â®', '`',  '<', 'â†’', 'Â°', 'â‚¬', 'â„¢', 'â€º',  'â™¥', 'â†', 'Ã—', 'Â§', 'â€³', 'â€²', 'Ã‚', 'â–ˆ', 'Â½', 'Ã ', 'â€¦', \n"," 'â€œ', 'â˜…', 'â€', 'â€“', 'â—', 'Ã¢', 'â–º', 'âˆ’', 'Â¢', 'Â²', 'Â¬', 'â–‘', 'Â¶', 'â†‘', 'Â±', 'Â¿', 'â–¾', 'â•', 'Â¦', 'â•‘', 'â€•', 'Â¥', 'â–“', 'â€”', 'â€¹', 'â”€', \n"," 'â–’', 'ï¼š', 'Â¼', 'âŠ•', 'â–¼', 'â–ª', 'â€ ', 'â– ', 'â€™', 'â–€', 'Â¨', 'â–„', 'â™«', 'â˜†', 'Ã©', 'Â¯', 'â™¦', 'Â¤', 'â–²', 'Ã¨', 'Â¸', 'Â¾', 'Ãƒ', 'â‹…', 'â€˜', 'âˆž', \n"," 'âˆ™', 'ï¼‰', 'â†“', 'ã€', 'â”‚', 'ï¼ˆ', 'Â»', 'ï¼Œ', 'â™ª', 'â•©', 'â•š', 'Â³', 'ãƒ»', 'â•¦', 'â•£', 'â•”', 'â•—', 'â–¬', 'â¤', 'Ã¯', 'Ã˜', 'Â¹', 'â‰¤', 'â€¡', 'âˆš','âˆž','Î¸','Ã·','Î±','â€¢','Ã ','âˆ’','Î²','âˆ…','Â³','Ï€','â€˜','â‚¹','Â´','Â°','â„¢','âˆšÂ²','â€”â€“' ]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2kQFP9hu9VdU"},"source":["punct_mapping = {\"â€˜\": \"'\", \"â‚¹\": \"e\", \"Â´\": \"'\", \"Â°\": \"\", \"â‚¬\": \"e\", \"â„¢\": \"tm\", \"âˆš\": \" sqrt \", \"Ã—\": \"x\", \"Â²\": \"2\", \"â€”\": \"-\", \"â€“\": \"-\", \"â€™\": \"'\", \"_\": \"-\", \"`\": \"'\", 'â€œ': '\"', 'â€': '\"', 'â€œ': '\"', \"Â£\": \"e\", 'âˆž': 'infinity', 'Î¸': 'theta', 'Ã·': '/', 'Î±': 'alpha', 'â€¢': '.', 'Ã ': 'a', 'âˆ’': '-', 'Î²': 'beta', 'âˆ…': '', 'Â³': '3', 'Ï€': 'pi', }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WXyuEHGr9aeC"},"source":["missplet_dict = {'didnt':'did not',\n"," \"i'm\": 'i am',\n"," 'doesnt' : 'does not',\n"," 'isnt' : 'is not',\n"," 'upvote' : 'up vote',\n"," 'wasnt' : 'was not',\n"," 'remindme': 'remind me',\n"," 'lt3' : 'love',\n"," 'upvotes' : 'up votes',\n"," 'shouldnt' : 'should not',\n"," 'hasnt' : 'has not',\n"," 'downvoted' : 'down voted',\n"," 'howd' : ' how do you do',\n"," 'upvoted' : 'up voted',\n"," 'rcasualconversation' : 'casual conversation',\n"," 'cakeday' : 'cake day',\n"," 'downvote' : 'down vote',\n"," 'whatre' : 'what are',\n"," 'contenderwhat' : 'contender what',\n"," 'downvotes' : 'down votes',\n"," 'heshe' : 'he she',\n"," 'tldr' : 'too long did not read',\n"," 'ftfy' : 'Fixed That For You',\n"," 'wbu' : 'what about you',\n"," 'thatd' : 'that would',\n"," 'welp' : 'well',\n"," 'ikr' : 'i know right',\n"," 'realise' : 'realize',\n"," 'thatll' : 'that will',\n"," 'upvoting' : 'up voting',\n"," 'whatd' : ' what did',\n"," 'grey': 'gray',\n"," 'yesno' : 'yes no',\n"," 'flavour' : 'flavor',\n"," 'tooslowly' : 'too slowly',\n"," 'realised' : 'realized',\n"," 'learnt' : 'learn',\n"," 'answersâ€”comment' :  'answers comment',\n"," 'gorillaz' : 'gorilla',\n"," 'â€œfuck' : 'fuck',\n"," 'accomplishedbeen' : 'accomplished been',\n"," 'whyd' : 'why did',\n"," 'edm' : 'electronic dance music',\n"," 'linkin' : 'linking',\n"," 'hbu' : 'how about you',\n"," 'mustve' : 'must have',\n"," 'whered' : 'where did',\n"," 'continuein' : 'continuing',\n"," 'welldo' : 'well done',\n"," 'downvoter' : 'down voter',\n"," 'friendo' : 'friend',\n"," 'â€œfixed' : 'fixed',\n"," 'yknow' : 'you know',\n"," 'programme' : 'program',\n"," 'heya' : 'hi',\n"," 'neighbours' : 'neighbor',\n"," 'hmu' : 'hit me up',\n"," 'twinsies' : 'twins',\n"," 'â€œwhat' : 'what',\n"," 'inb4' : 'in before',\n"," 'halflife' : 'half life',\n"," 'heyy' : 'hey',\n"," 'summarise' : 'summarize',\n"," 'lpt' : 'life pro tip',\n"," 'gofundme' : 'go find me',\n"," '5yearold' : '5 year old',\n"," 'noyou' : 'no you',\n"," 'yesyou' : 'yes you',\n"," 'life\".' : 'life',\n"," 'yess' : 'yes',\n"," 'greys' : 'gray',\n"," 'daynight' : 'day night',\n"," 'yessss' : 'yes',\n"," 'trickortreating' : 'trick or treat',\n"," 'ninenine' : 'nine nine',\n"," 'dnd' : 'do not disturb',\n"," 'howre' : 'how are',\n"," 'highfive' : 'high five',\n"," 'favour' : 'favor',\n"," 'gtfo' : 'Get the fuck out',\n"," 'necessarythey' : 'necessary they',\n"," 'yessssss' : 'yes',\n"," 'jaja' : 'haha',\n"," 'stopdinner' : 'stop dinner',\n"," 'wristbander' :  'wrist bander',\n"," 'tf2' : 'team fortress',\n"," 'ptsd' : 'post traumatic stress disorder',\n"," 'whataburger' : 'what a burger',\n"," 'youi' : 'you i',\n"," 'nosleep' : 'no sleep',\n"," 'warframe' : 'war frame',\n"," 'what?*' : 'what',\n"," 'yoooo' : 'yes',\n"," 'lul' : 'love you lots',\n"," 'pmed' : 'private messaged',\n"," ':&gt' : 'get through',\n"," 'pmd' : 'private messaged',\n"," 'ahahah' : 'haha',\n"," 'rsuddenlygay' : 'suddenly gay',\n"," 'rsuicidebywords' : 'suicide by words',\n"," 'rmadlads' :'mad lads',\n"," 'â€œoh' : 'oh',\n"," 'iâ€˜m' : 'i am',\n"," 'raww' : 'raw',\n"," 'clickbait' : 'click bait',\n"," 'rshowerthoughts' : 'shower thoughts',\n"," 'ymmv' : 'your mileage may vary',\n"," 'imnsho' : 'in my not so humble opinion',\n"," 'shittttt' : 'shit',\n"," 'holllyyyyyyy' : 'holy',\n"," 'foxygen' : 'oxygen',\n"," 'rteenagers' : 'teenagers',\n"," 'gotchu' : 'got you',\n"," 'boredwho' : 'bored who',\n"," 'gt;: ' : 'get through',\n"," 'coldplay' : 'cold play',\n"," 'crosspost' : 'cross post',\n"," ' #tellyourstoryin5words' : 'tell your story in 5 words',\n"," 'isare' : 'is are',\n"," 'sfw' : 'safe for work',\n"," 'downvoting' :  'down voting',\n"," 'rpointlessstories' : 'pointless stories',\n"," 'now?!' : 'now',\n"," 'apologise' : 'apologize',\n"," 'â€œthey' : 'they',\n"," 'enoughâ€™.' : 'enough',\n"," 'goodcool' : 'good cool',\n"," 'gtgt': 'got to go to',\n"," 'â€˜not' : 'not',\n"," 'upvoter' : 'up voter',\n"," 'breathtakingyour' :  'breath taking our',\n"," 'runexpectedoffice' : 'unexpected office',\n"," 'leavingbi' : 'leaving',\n"," 'rfoundthemobileuser' : 'found the mobile user',\n"," 'thanksif' : 'thanks if',\n"," 'rwholesome' : 'wholesome',\n"," 'lol' : 'laugh out loud',\n"," 'rlounge' : 'lounge',\n"," 'soundcloud' : 'sound cloud',\n"," 'leppard' : 'leopard',\n"," 'rtotallynotrobots' : 'totally not robots',\n"," 'cavetown' : 'cave town',\n"," 'blinddisabled' : 'blind disabled',\n"," 'introverting' : 'introvert',\n"," 'lmaoooo' : 'laughing my ass off',\n"," 'yh' : 'yes',\n"," 'sucks!\"america' : 'sucks america',\n"," 'fawlty' : 'faulty',\n"," 'runpopularopinion' : 'run popular opinion',\n"," 'yesssss' : 'yes',\n"," 'crossdress' : 'cross dress',\n"," 'adminsemployees' : 'admins employees',\n"," 'smokedis' : 'smoke',\n"," 'spelt' : 'spell',\n"," 'â€œfar' : 'far',\n"," 'dayweekmonth' : 'day week month',\n"," 'thicc' : 'thick',\n"," 'yaaay' : 'yes',\n"," 'suckswhats' : 'sucks what',\n"," 'durnk' : 'drunk',\n"," 'butterfinger' : 'butter finger',\n"," 'tekken' : 'taken',\n"," 'yayyyy' : 'yes',\n"," 'recall': 'recall',\n"," 'lotr' : 'lord of the rings',\n"," 'mhm' : 'yes',\n"," 'travelled' : 'travel',\n"," 'lolyou' : 'laugh out loud you',\n"," 'rlifeprotips' : 'life pro tips',\n"," 'itwhat' : 'it what',\n"," 'rwallstreetbets' : 'wall street bets',\n"," 'sharptooth' : 'sharp tooth',\n"," 'vsauce' : 'sauce',\n"," 'yayyy' : 'yes',\n"," 'casualconversation' : 'casual conversation',\n"," 'world war 2' : 'world war 2',\n"," 'hiiii' : 'hi',\n"," 'no3' : 'no',\n"," 'selfie' : 'selfie',\n"," 'dreamworks' : 'dream works',\n"," 'bf4' : 'before',\n"," 'hej' : 'hi',\n"," 'flavoured' : 'flavored',\n"," 'daaamn' : 'damn',\n"," 'son' : 'son',\n"," 'you' : 'you',\n"," 'gay\".' : 'gay',\n"," 'nword' : 'word',\n"," 'rthedavincicode' : 'the vinci code',\n"," 'brexit' : 'brexit',\n"," 'midtwenties' : 'mid twenties',\n"," 'bingewatch' : 'binge watch',\n"," 'peelâ€.' : 'peel',\n"," 'yourselfwell' : 'yourself well',\n"," 'yppah' : 'yes',\n"," 'rbisexual' : 'bisexual',\n"," 'kindda' : 'kind of',\n"," 'runderratedcomments' : 'underrated comments',\n"," 'backmaybe' : 'back may be',\n"," 'itbut' : 'it but',\n"," 'suggestionsi' : 'suggestions',\n"," 'motherfuckerglad' : 'mother fucker glad',\n"," 'nvm' : 'never mind',\n"," 'songshelp' : 'songs help',\n"," 'positivityfun' : 'positivity fun',\n"," 'indoorone' : 'indore one',\n"," 'timehow' : 'time how',\n"," 'lolbut' : 'laugh out loud but',\n"," 'manwoman' : 'man woman',\n"," 'feelingdoing' : 'feeling doing',\n"," 'friendscould' : 'friends cloud',\n"," 'rhappyrelationships' : 'happy relationships',\n"," 'do. Lonely' : 'do lonely',\n"," 'realisation' : 'realization',\n"," 'woooow' : 'wow',\n"," 'rcatsstandingup' : 'cats standing up',\n"," 'goldenboye' : 'golden boy',\n"," 'weeki' : 'week',\n"," 'numbah' : 'number',\n"," 'yeahh' : 'yes',\n"," 'youim' : 'you i am',\n"," 'toowhat' : 'too what',\n"," 'deffinetly' : 'definitely',\n"," 'game3' : 'game',\n"," 'xpost' : 'post',\n"," 'rchildfree' : 'child free',\n"," 'music2' : 'music',\n"," 'quotebook' : 'quote book',\n"," 'ufuckswithducks' : 'fuck with duck',\n"," 'lold' : 'old',\n"," 'whaaaaat' : 'what',\n"," 'flavours' : 'flavors',\n"," 'rpics' : 'pictures',\n"," 'piano2' : 'piano',\n"," 'requirementsi' : 'requirements',\n"," 'himher' : 'him her',\n"," 'yeaaah' : 'yes',\n"," 'rsuicidewatch' : 'suicide watch',\n"," 'damnnnn' : 'damn',\n"," 'longsword' : 'long word',\n"," 'awfuli' : 'awefully',\n"," 'humour' : 'humor',\n"," 'fooddrink' : 'food drink',\n"," 'hby' : 'how about you',\n"," 'roomwhat' : 'room what',\n"," 'selfies' : 'selfie',\n"," 'feelsbadman' : 'feels bad man',\n"," 'youuu' : 'you',\n"," 'mustnt' : 'must not',\n"," 'nederlands' : 'netherlands',\n"," 'aaaaand' : 'and',\n"," 'smoll' : 'small',\n"," 'yus' : 'yes',\n"," 'madlad' : 'mad lad',\n"," 'rniceguys' : 'nice guys',\n"," 'uyourearealcunt' : ' you are a real cunt',\n"," 'rimsorryjon' : 'i am sorry jon',\n"," 'yet.\"' : 'yet',\n"," 'editðŸ˜¬' : 'edit',\n"," 'retreival' : 'retrieval',\n"," 'haaaaate' : 'hate',\n"," 'musicianband' : 'musician band',\n"," 'wingies' : 'wings',\n"," 'money4' : 'money',\n"," 'work2' : 'work',\n"," 'goodi' : 'good',\n"," 'touchstarved' : 'touch starved',\n"," 'grindr' : 'grinder',\n"," 'rblunderyears' : 'blunder years',\n"," 'rcursedcomments' : 'cursed comments',\n"," 'â€˜em' : 'them',\n"," 'rstaticsnake' : 'static snake',\n"," 'marcy' : 'mercy',\n"," 'mightve' : 'might have',\n"," 'cuteits' : 'cute',\n"," 'whatim' : 'what i am',\n"," 'havingfun' : 'having fun',\n"," 'exwifes' : 'ex wives',\n"," 'noyoure' : 'no you are',\n"," 'aaaaand' : 'and',\n"," 'smoll' : 'small',\n"," 'yus' : 'yes',\n"," \"i'hv\": \"i have\",\n"," \"i'll\": \"i will\",\n"," 'â˜ºï¸':'ðŸ˜Š',\n"," 'ampx###b': 'amp bitch',\"i've\": 'i have','rwholesomememes':'wholesome memes',\n"," 'helloðŸ˜€ðŸ˜Šâ˜ºðŸ™ŒðŸ‘':'hello ðŸ˜€ ðŸ˜Š','ÍœÊ–': 'ðŸ˜Š', 'ðŸ˜‚ðŸ˜‚':'ðŸ˜‚','ðŸ˜‚ðŸ˜‚ðŸ˜‚':'ðŸ˜‚','ayyyy':'yes','\\U0001f970':'ðŸ˜Š', 'ðŸ…±ï¸ailure' : 'big failure', \"goin'\": 'going','ðŸ‘‰ðŸ˜ŽðŸ‘‰': 'ðŸ˜Ž', \"d'aww\" : 'so cute','ulionghost': ' you lion ghost',\"fuckin'\": 'fucking', 'uiwinalot7': ' you i win a lot', 'goodra': 'good',\n","'pie5': 'being soft', 'ðŸ˜€ðŸ˜Šâ˜º': 'ðŸ˜€', 'uwaterguy##': ' you water guy', ' rseriousconversation': 'serious conversation','runexpectedfactorial':'unexpected factorial', \"i'mma\": 'i am',\n","'ðŸ‘ðŸ‘':'ðŸ‘', \"lol'd\": \"laughed out loud\", 'à² à² ': 'ðŸ˜Ž', 'rtea': 'tea', 'ðŸ¤£ðŸ¤£ðŸ¤£': 'ðŸ¤£','ðŸ‘ðŸ‘ðŸ‘':'ðŸ‘',\"you'\": 'you', \"d'aw\": 'so cute', 'pmmeyogurtpics':'send me yogurt pictures',\n","\"sirma'am\": 'sir madam','rusernamechecksout': 'user name checks out', 'lt3lt3lt3': 'love', 'bookmovie': 'book movie', 'ðŸ˜€ðŸ˜Šâ˜ºðŸ¤—ðŸ˜‡': 'ðŸ˜€ ðŸ˜Š ðŸ¤— ðŸ˜‡',\n","'rpopping': 'popping','gt##': 'got to go', 'rwoooosh': 'woooosh', 'rr4r': 'redditor for redditor','ustaticsnake': 'you static snake','eli5':'explain in simple words',\n","'mrbeast': 'beast', 'rkarmaroulette':'karma roulette','ðŸ˜­ðŸ˜­':'ðŸ˜­','ï¸\\U0001f9e1ðŸ’›ðŸ’šðŸ’™ðŸ’œ':'love','ðŸ¤”ðŸ¤”ðŸ¤”':'ðŸ¤”','rgatekeeping':'gate keeping', 'moviestv': 'movie television','dinozo':'dinosaur', 'scarn': 'mayhem',\n","'\\U0001f9e1ðŸ’›ðŸ’šðŸ’™ðŸ’œ': 'love', \"pm'd\":'messaged','ayyye':'yes', '\\U0001f92a':'ðŸ˜€', 'gtno': 'get the fuck out', \"walkin'\": 'walking', 'rknightsofpineapple':'knights of pineapple',\n","'##what': 'what','rbrandnewsentence':'brand new sentence', 'bhosadiwale':'son of a bitch', 'runexpectedthanos': 'unexpected thanos',\"'murica\": 'america',\n","'udonaldduck':'you donald duck','rjobs':'jobs' ,'uitsthesnake': 'it is the snake','ðŸ’•ðŸ’•': 'ðŸ’•'}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MpgtDLuA9cln"},"source":["def clean_contractions(text, mapping):\n","    specials = [\"â€™\", \"â€˜\", \"Â´\", \"`\"]\n","    for s in specials:\n","        text = text.replace(s, \"'\")\n","    text = ' '.join([mapping[t] if t in mapping else mapping[t.lower()] if t.lower() in mapping else t for t in text.split(\" \")])\n","    return text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qWc5d-6S9fzo"},"source":["def clean_special_chars(text, punct, mapping):\n","    for p in mapping:\n","        text = text.replace(p, mapping[p])\n","    \n","    for p in punct:\n","        text = text.replace(p, '')\n","    \n","    specials = {'\\u200b': ' ', 'â€¦': ' ', '\\ufeff': ''}  # to be updated again upon checking the coverage\n","    for s in specials:\n","        text = text.replace(s, specials[s])\n","    \n","    return text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AHqnZ6pl9hx1"},"source":["def remove_newlines(sent):\n","  sent = re.sub(r'\\s+', \" \", sent )\n","  return sent"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vGE2_rXC9lEg"},"source":["def clean_numbers(x):\n","    if bool(re.search(r'\\d', x)):\n","        x = re.sub('[0-9]{5,}', '#####', x)\n","        x = re.sub('[0-9]{4}', '####', x)\n","        x = re.sub('[0-9]{3}', '###', x)\n","        x = re.sub('[0-9]{2}', '##', x)\n","        #x = re.sub('[0-9]', '#', x)\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v325feRi9m31"},"source":["def clean_missplets(text, mapping):\n","    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n","    return text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TJyP7YhI9o5s"},"source":["# fetching data from csv file to dataframe\n","reddit = pd.read_csv('/content/drive/My Drive/Data/Conversations/casual_data_windows.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":111},"id":"javbzo7q9sEr","executionInfo":{"status":"ok","timestamp":1623151723184,"user_tz":-330,"elapsed":24,"user":{"displayName":"ninja turtle","photoUrl":"","userId":"08410201356610850962"}},"outputId":"ec8dcb69-0279-4b67-caae-091a0a7034fd"},"source":["reddit.head(2)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>What kind of phone(s) do you guys have?</td>\n","      <td>I have a pixel. It's pretty great. Much better...</td>\n","      <td>Does it really charge all the way in 15 min?</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>I have a pixel. It's pretty great. Much better...</td>\n","      <td>Does it really charge all the way in 15 min?</td>\n","      <td>Pretty fast. I've never timed it, but it's und...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0  ...                                                  2\n","0           0  ...       Does it really charge all the way in 15 min?\n","1           1  ...  Pretty fast. I've never timed it, but it's und...\n","\n","[2 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"LwLlQV0D9u71"},"source":["# creating an empty dataframe to hold consecutive conversation-pairs\n","df_conv = pd.DataFrame(columns=['sent1', 'sent2'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MY3yxa419zJ1"},"source":["# filling the dataframe with conversation pairs from reddit dataframe\n","for index, row in reddit.iterrows():\n","    df_conv = df_conv.append({'sent1':row[\"0\"],'sent2':row[\"1\"]},ignore_index=True)\n","    df_conv = df_conv.append({'sent1':row[\"1\"],'sent2':row[\"2\"]},ignore_index=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tEefRn9C9-bA"},"source":["df_conv['sent1'] = df_conv['sent1'].apply(lambda x: clean_contractions(x, contraction_mapping))\n","df_conv['sent2'] = df_conv['sent2'].apply(lambda x: clean_contractions(x, contraction_mapping))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B4aH-AJp-CBZ"},"source":["df_conv['sent1'] = df_conv['sent1'].apply(lambda x: clean_special_chars(x, punct, punct_mapping))\n","df_conv['sent2'] = df_conv['sent2'].apply(lambda x: clean_special_chars(x, punct, punct_mapping))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7fMl6JlN-E-_"},"source":["df_conv['sent1'] = df_conv['sent1'].apply(lambda x: clean_contractions(x, contraction_mapping))\n","df_conv['sent2'] = df_conv['sent2'].apply(lambda x: clean_contractions(x, contraction_mapping))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i7vVWYNa-HhM"},"source":["df_conv['sent1'] = df_conv['sent1'].apply(lambda x : x.lower())\n","df_conv['sent2'] = df_conv['sent2'].apply(lambda x : x.lower())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JaL5wt11-Jpa"},"source":["df_conv['sent1'] = df_conv['sent1'].apply(lambda x : remove_newlines(x))\n","df_conv['sent2'] = df_conv['sent2'].apply(lambda x : remove_newlines(x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vsCviykK-MKT"},"source":["df_conv['sent1'] = df_conv['sent1'].apply(lambda x: clean_numbers(x))\n","df_conv['sent2'] = df_conv['sent2'].apply(lambda x: clean_numbers(x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xA2ISul5-O3e"},"source":["df_conv['sent1'] = df_conv['sent1'].apply(lambda x: clean_numbers(x))\n","df_conv['sent2'] = df_conv['sent2'].apply(lambda x: clean_numbers(x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tBP3lYI3-R8O"},"source":["df_conv['sent1'] = df_conv['sent1'].apply(lambda x: clean_missplets(x, missplet_dict))\n","df_conv['sent2'] = df_conv['sent2'].apply(lambda x: clean_missplets(x, missplet_dict))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":221},"id":"d2FTx_SU-U5_","executionInfo":{"status":"ok","timestamp":1623152285609,"user_tz":-330,"elapsed":61,"user":{"displayName":"ninja turtle","photoUrl":"","userId":"08410201356610850962"}},"outputId":"1f5c03d9-59c0-472e-ccfc-16153ca807e2"},"source":["print(df_conv.shape)\n","df_conv.head()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(112594, 2)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sent1</th>\n","      <th>sent2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>what kind of phones do you guys have</td>\n","      <td>i have a pixel it is pretty great much better ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>i have a pixel it is pretty great much better ...</td>\n","      <td>does it really charge all the way in ## min</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>i have a pixel it is pretty great much better ...</td>\n","      <td>does it really charge all the way in ## min</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>does it really charge all the way in ## min</td>\n","      <td>pretty fast i have never timed it but it is un...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>does it really charge all the way in ## min</td>\n","      <td>pretty fast i have never timed it but it is un...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                               sent1                                              sent2\n","0               what kind of phones do you guys have  i have a pixel it is pretty great much better ...\n","1  i have a pixel it is pretty great much better ...        does it really charge all the way in ## min\n","2  i have a pixel it is pretty great much better ...        does it really charge all the way in ## min\n","3        does it really charge all the way in ## min  pretty fast i have never timed it but it is un...\n","4        does it really charge all the way in ## min  pretty fast i have never timed it but it is un..."]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"id":"sGdfsQ0n-XeO"},"source":["duplicateRowsDF = df_conv[df_conv.duplicated()]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xlCTYMhf-ang"},"source":["# removing duplicate rows\n","df_conv = df_conv.drop_duplicates(subset=None, keep='first', inplace=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":221},"id":"geaLbZBm-cr3","executionInfo":{"status":"ok","timestamp":1623152285613,"user_tz":-330,"elapsed":45,"user":{"displayName":"ninja turtle","photoUrl":"","userId":"08410201356610850962"}},"outputId":"341adda6-e1d8-4984-c5f2-76dc55d0f7f5"},"source":["print(df_conv.shape)\n","df_conv.head()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(71052, 2)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sent1</th>\n","      <th>sent2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>what kind of phones do you guys have</td>\n","      <td>i have a pixel it is pretty great much better ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>i have a pixel it is pretty great much better ...</td>\n","      <td>does it really charge all the way in ## min</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>does it really charge all the way in ## min</td>\n","      <td>pretty fast i have never timed it but it is un...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>pretty fast i have never timed it but it is un...</td>\n","      <td>cool i have been thinking of getting one my ph...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>what kind of phones do you guys have</td>\n","      <td>samsung galaxy j1 it is my first cell phone an...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                               sent1                                              sent2\n","0               what kind of phones do you guys have  i have a pixel it is pretty great much better ...\n","1  i have a pixel it is pretty great much better ...        does it really charge all the way in ## min\n","3        does it really charge all the way in ## min  pretty fast i have never timed it but it is un...\n","5  pretty fast i have never timed it but it is un...  cool i have been thinking of getting one my ph...\n","6               what kind of phones do you guys have  samsung galaxy j1 it is my first cell phone an..."]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"id":"TMrYz43K-fAR"},"source":["# filtering out unique sentence set from conversation pairs. this is needed to be done before creating word to index dictionary\n","\n","encoder_data = df_conv['sent1']\n","decoder_data = df_conv['sent2']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cd-RFNGx-i03","executionInfo":{"status":"ok","timestamp":1623152285616,"user_tz":-330,"elapsed":38,"user":{"displayName":"ninja turtle","photoUrl":"","userId":"08410201356610850962"}},"outputId":"df5c6ce7-d006-499f-ede0-5b9f5d322340"},"source":["# Series of unique sentences\n","unique_sents = decoder_data.append(encoder_data[:1])\n","len(unique_sents)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["71053"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"markdown","metadata":{"id":"skaVBc1UJKXv"},"source":["# Embedding"]},{"cell_type":"code","metadata":{"id":"HGzdowvX-lIy"},"source":["sentences_tok = []\n","for sent_str in unique_sents.values:\n","    tokens = sent_str.split()\n","    sentences_tok.append(tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OOySCUJfAYR0","executionInfo":{"status":"ok","timestamp":1623152446371,"user_tz":-330,"elapsed":396,"user":{"displayName":"ninja turtle","photoUrl":"","userId":"08410201356610850962"}},"outputId":"f3be68f8-46b6-4d17-c4fb-11e17a7b9d47"},"source":["sentences_tok[5:6]\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['what',\n","  'do',\n","  'you',\n","  'think',\n","  'of',\n","  'it',\n","  'anything',\n","  'you',\n","  'do',\n","  'not',\n","  'like']]"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"ewdVd6DOCJVu"},"source":["from gensim.models import FastText\n","model_fast = FastText(sentences_tok, size=300, window=5, min_count=1, workers=4,sg=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rx_RGYirC4Nm","executionInfo":{"status":"ok","timestamp":1623152516338,"user_tz":-330,"elapsed":333,"user":{"displayName":"ninja turtle","photoUrl":"","userId":"08410201356610850962"}},"outputId":"e17579ec-6d08-404b-b66b-473cb1665976"},"source":["model_fast.wv['you']"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([-1.89220861e-01,  2.94760287e-01, -4.83378410e-01, -6.67391792e-02,\n","       -2.98199743e-01, -5.30537605e-01,  2.86983280e-03, -2.20747218e-01,\n","        2.91514486e-01, -1.10376872e-01, -2.48871483e-02, -5.25510497e-02,\n","       -7.50873908e-02,  1.08912639e-01, -3.52990597e-01,  1.25542417e-01,\n","        5.85989170e-02,  1.82362810e-01,  2.66778208e-02, -1.18527018e-01,\n","       -2.25809649e-01,  1.96421370e-01,  8.64098407e-03,  1.30207185e-02,\n","        3.52608897e-02,  2.62196194e-02, -4.14143711e-01, -2.66967803e-01,\n","       -1.36522442e-01, -5.65856881e-02, -2.40269825e-01, -7.51627684e-02,\n","        5.58274053e-02,  5.17799929e-02,  1.88356787e-01, -3.00463527e-01,\n","       -8.28464255e-02,  9.94886607e-02, -5.72060347e-02,  6.98634461e-02,\n","        2.45434901e-04,  1.71174537e-02, -1.80396009e-02,  3.39121580e-01,\n","        2.86912441e-01,  7.77478069e-02,  2.44166225e-01, -1.68209314e-01,\n","        3.04887682e-01, -2.97264308e-01, -1.29068613e-01,  2.90176570e-01,\n","       -4.01426256e-01, -6.24021590e-02,  2.68253714e-01, -2.26159677e-01,\n","       -1.51224909e-02, -4.21755403e-01,  1.13670662e-01,  1.00906953e-01,\n","       -1.57526001e-01,  2.58518755e-03,  2.69902974e-01,  1.77163824e-01,\n","       -1.94564462e-01,  1.15351625e-01,  5.96298985e-02, -1.43821361e-02,\n","        1.40578672e-01, -1.22011445e-01,  2.00343892e-01, -3.74052674e-01,\n","       -1.84215143e-01, -6.66167587e-02, -2.43883878e-02, -9.20804068e-02,\n","       -3.55585031e-02, -2.54147589e-01,  1.64729714e-01,  7.52200605e-03,\n","       -3.70863974e-01, -6.62490055e-02, -8.62445161e-02, -9.29888263e-02,\n","       -1.37028601e-02, -1.64992109e-01,  1.58242509e-02,  3.54845613e-01,\n","        2.58981764e-01,  9.74303856e-02,  7.93882534e-02,  1.52483091e-01,\n","       -1.04907744e-01, -4.75208253e-01,  2.30337083e-02, -2.02117726e-01,\n","       -1.71465486e-01,  6.49551302e-02, -1.16783917e-01,  2.13679716e-01,\n","       -2.00671405e-01,  1.05726235e-01,  1.80741176e-01,  4.24479991e-01,\n","        5.49725413e-01, -8.77820328e-02, -4.33628857e-01,  1.27683237e-01,\n","        2.35901903e-02,  7.40188286e-02,  1.92426801e-01,  1.52273308e-02,\n","        2.72715718e-01, -1.60912909e-02, -9.05568227e-02,  2.00260669e-01,\n","       -5.96720353e-02, -2.05986667e-02,  1.18281886e-01, -9.80882645e-02,\n","       -2.36189872e-01, -1.02971569e-02, -2.26907164e-01, -1.35622444e-02,\n","       -1.72650099e-01, -1.55059054e-01, -9.53812152e-02,  2.53357917e-01,\n","       -1.61128253e-01,  1.42761126e-01, -2.06712317e-02, -1.36225939e-01,\n","       -3.01995099e-01, -1.74611226e-01,  2.00402141e-01,  6.92931644e-04,\n","        3.10737103e-01, -3.19207579e-01,  4.75948811e-01,  2.82785833e-01,\n","        1.90399185e-01, -1.72913343e-01, -5.41005619e-02, -2.73238003e-01,\n","       -1.37035221e-01, -1.65187851e-01,  1.48736194e-01,  7.16865808e-02,\n","        2.21677944e-01, -3.12861383e-01, -1.38106272e-01,  1.40295047e-02,\n","        6.77441154e-03,  1.25294670e-01, -7.60518983e-02,  9.76068247e-03,\n","       -2.10238233e-01, -7.22966418e-02,  2.53084004e-01,  2.60608375e-01,\n","       -1.09790213e-01,  7.76380720e-03,  8.36748257e-02,  1.33786589e-01,\n","       -4.19789463e-01, -3.89510440e-03,  1.90733209e-01, -1.88146576e-01,\n","       -1.97060719e-01,  2.42286965e-01, -5.04856044e-03,  2.36668468e-01,\n","       -2.09675834e-01,  1.30636737e-01, -3.22585821e-01, -1.91534415e-01,\n","       -9.19522718e-02, -1.44977897e-01, -6.18376322e-02,  2.65771925e-01,\n","        1.61364581e-02,  2.76096929e-02,  9.10747498e-02, -1.67209670e-01,\n","       -3.01173359e-01,  1.68380514e-01, -1.55922145e-01, -1.21817756e-02,\n","        2.82852113e-01, -1.88440140e-02, -2.66714483e-01, -4.80666980e-02,\n","       -1.46123126e-01, -1.54079303e-01,  4.96084690e-01,  4.93712872e-01,\n","        5.79872057e-02,  1.40467837e-01,  1.16352148e-01, -3.48085575e-02,\n","       -8.55732113e-02, -1.36776641e-01,  9.17142108e-02,  7.88601190e-02,\n","        5.54174483e-02,  1.63958028e-01,  1.00218719e-02, -6.60790727e-02,\n","        1.17327876e-01,  5.53725302e-01,  2.76580714e-02,  1.17458798e-01,\n","        5.65326214e-01, -2.45687962e-01, -3.05575103e-01,  4.17440198e-02,\n","       -5.52323699e-01, -3.95597965e-01,  1.42801166e-01,  4.89202794e-03,\n","       -1.77979916e-01, -1.19869091e-01,  2.23577052e-01,  1.47315934e-01,\n","       -1.22475080e-01,  6.01554811e-01,  2.58019179e-01, -9.49979480e-03,\n","        1.04314871e-01, -3.85284424e-01, -1.87802166e-01,  8.82175937e-02,\n","        2.23218709e-01, -2.77648658e-01,  3.74072701e-01,  2.91836411e-01,\n","       -4.27216850e-02,  1.79824427e-01, -2.09517196e-01, -1.22391604e-01,\n","       -4.43857104e-01, -8.78550410e-02, -2.70334691e-01,  3.77380177e-02,\n","       -1.76974863e-01, -2.34413706e-02, -2.02821255e-01,  1.30793815e-02,\n","        1.23940587e-01, -9.19503421e-02, -8.57607350e-02,  4.86432165e-01,\n","        1.43164128e-01,  1.63573906e-01, -2.88639218e-02,  9.45551544e-02,\n","        1.84581488e-01,  1.10612221e-01,  2.28892639e-01, -1.41647145e-01,\n","        5.73873892e-03,  3.13977182e-01, -2.03475401e-01, -7.16898590e-02,\n","       -1.93996623e-01, -1.11529775e-01,  1.66634172e-01, -1.17514752e-01,\n","       -2.41103899e-02, -6.61958009e-02,  3.77733037e-02,  2.48971298e-01,\n","       -5.41052744e-02, -1.75137985e-02, -4.44884077e-02,  7.56073743e-02,\n","       -3.86155814e-01, -1.20221533e-01, -2.12356243e-02, -7.30577530e-03,\n","        6.21180892e-01,  5.57819903e-02, -2.23816350e-01,  3.21089268e-01,\n","       -2.53208041e-01, -1.34681568e-01,  2.49545768e-01,  8.07812065e-02,\n","       -1.06245175e-01,  2.55691260e-01, -2.80469865e-01,  3.96109819e-01,\n","        5.96047565e-02, -1.25986204e-01, -4.11954314e-01, -2.40312099e-01,\n","       -3.70858349e-02,  4.54097725e-02,  4.41663228e-02, -2.11540192e-01],\n","      dtype=float32)"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"code","metadata":{"id":"S0gMvmpdFR5P"},"source":["from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h766ilqTFR13"},"source":["tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(list(unique_sents.values))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s6Ijw2G9FiOi"},"source":["# word to index dictionary is created using keras Tokenizer class\n","word_index = tokenizer.word_index"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LZizbKT-Hgiu"},"source":["# Writing the custom embedding to a text file from which torchtext will read the embeddings and them into vector object later\n","\n","with open('/content/drive/My Drive/Data/custom_fasttext_embeddings.txt', 'w+') as f:\n","  for token, vector in word_index.items():\n","    vector = model_fast.wv[token]\n","    vector_str = ' '.join([str(v) for v in vector])\n","    f.write(f'{token} {vector_str}\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zUAl3tEAJWIA"},"source":["# Torchtext"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sW5Zq2RCH4mn","executionInfo":{"elapsed":4626,"status":"ok","timestamp":1615454289985,"user":{"displayName":"ninja turtle","photoUrl":"","userId":"08410201356610850962"},"user_tz":-330},"outputId":"563d4901-e19c-4473-bcb2-1089ae2b4d22"},"source":["# spaCy has model for each language which is needed to be loaded so we can access the tokenizer of that language's model\n","\n","# downloading english language model\n","\n","!python -m spacy download en"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n","Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (54.0.0)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n","Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.1)\n","\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('en_core_web_sm')\n","\u001b[38;5;2mâœ” Linking successful\u001b[0m\n","/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n","/usr/local/lib/python3.7/dist-packages/spacy/data/en\n","You can now load the model via spacy.load('en')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BAQpT9FDIl4G"},"source":["english = spacy.load('en')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WxozwQC7KtqI"},"source":["def tokenize_sent1(sentence):\n","  #return [tok.text for tok in english.tokenizer(sentence)]  #not reversing\n","  return [tok.text for tok in english.tokenizer(sentence)][::-1]\n","\n","def tokenize_sent2(sentence):\n","  return [tok.text for tok in english.tokenizer(sentence)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w6iidpw6Kybh"},"source":["# batch_first: Whether to produce tensors with the batch dimension first.\n","# by default batch_first is False, but the transformer should be fed examples with the batch first ([batch_size, seq_len]) \n","# thus making the attribute value True\n","\n","sent1 = Field(tokenize = tokenize_sent1,init_token='<sos>',eos_token='<sos>',batch_first = True)\n","sent2 = Field(tokenize = tokenize_sent2,init_token='<sos>',eos_token='<sos>',batch_first = True) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vbP9I8FmK25s"},"source":["train_data , test_data = train_test_split(df_conv,test_size = 0.2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rX526VJXSfGD","executionInfo":{"elapsed":669,"status":"ok","timestamp":1615454298395,"user":{"displayName":"ninja turtle","photoUrl":"","userId":"08410201356610850962"},"user_tz":-330},"outputId":"5bb1b033-a94b-42ef-a9b5-bceff1b36c24"},"source":["%cd '/content/drive/My Drive/Data/Conversations/'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Data/Conversations\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bX3mXS81SiDL"},"source":["train_data.to_csv('train.csv',index = False)\n","test_data.to_csv('test.csv',index = False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cAUfmGECSksO"},"source":["data_fields = [('sent1',sent1),('sent2',sent2)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JZumviYvSoK5"},"source":["# TabularDataset(Dataset):Defines a Dataset of columns stored in CSV, TSV, or JSON format.\n","# Create a TabularDataset given a path, file format, and field list\n","\n","train_data , test_data = TabularDataset.splits(path ='/content/drive/My Drive/Data/Conversations/' ,train='train.csv', test ='test.csv', format='csv', fields=data_fields)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hqoBISVCSrV7","executionInfo":{"elapsed":728,"status":"ok","timestamp":1615454313191,"user":{"displayName":"ninja turtle","photoUrl":"","userId":"08410201356610850962"},"user_tz":-330},"outputId":"eb026fcc-5bb8-472b-9723-d977ac6fe105"},"source":["print(f\"Number of training examples: {len(train_data.examples)}\")\n","print(f\"Number of validation examples: {len(test_data.examples)}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of training examples: 56842\n","Number of validation examples: 14212\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SbvHqV3SSu5Z","executionInfo":{"elapsed":537,"status":"ok","timestamp":1615454314567,"user":{"displayName":"ninja turtle","photoUrl":"","userId":"08410201356610850962"},"user_tz":-330},"outputId":"9cabe4bf-91e7-4d1c-af20-e5333ad1f3b0"},"source":["print(vars(train_data.examples[1]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'sent1': ['food', 'have', 'to', 'deserve', 'polite', 'are', 'who', 'people', 'only', 'do', 'why'], 'sent2': ['fuck', 'off', 'with', 'your', 'white', 'knight', 'bullshit']}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TV_6hGfJSz2G","executionInfo":{"elapsed":515,"status":"ok","timestamp":1615454315899,"user":{"displayName":"ninja turtle","photoUrl":"","userId":"08410201356610850962"},"user_tz":-330},"outputId":"c490055d-35c4-4028-9746-4f8e4abc5fe8"},"source":["print(vars(test_data.examples[1]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'sent1': ['username', 'your', 'behind', 'story', 'the', 'is', 'what'], 'sent2': ['i', 'got', 'this', 'from', 'that', 'stupid', 'got', \"'\", 'eeeem', 'vine']}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"S5XEocY4S2ev"},"source":["import torchtext.vocab as vocab\n","\n","# reading the custom embedding wriiten on a text file with TorchText and converting them to vector objects\n","\n","custom_embeddings = vocab.Vectors(name = '/content/drive/My Drive/Data/custom_fasttext_embeddings.txt')\n","sent1.build_vocab(train_data, vectors = custom_embeddings) \n","sent2.build_vocab(train_data, vectors = custom_embeddings)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"In2VpcSzS42Q"},"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6JBX1UAbTLBw","executionInfo":{"elapsed":737,"status":"ok","timestamp":1615454322036,"user":{"displayName":"ninja turtle","photoUrl":"","userId":"08410201356610850962"},"user_tz":-330},"outputId":"1fb6908c-e970-4ac9-beb2-0981ec7e5878"},"source":["device"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cpu')"]},"metadata":{"tags":[]},"execution_count":54}]},{"cell_type":"code","metadata":{"id":"x0S3GyHjTL7V"},"source":["BATCH_SIZE = 150\n","\n","train_iterator, valid_iterator = BucketIterator.splits(\n","    (train_data, test_data),\n","    batch_size = BATCH_SIZE, \n","    device = device,\n","    sort=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vLsH4-rrTQEt","executionInfo":{"elapsed":509,"status":"ok","timestamp":1615454324328,"user":{"displayName":"ninja turtle","photoUrl":"","userId":"08410201356610850962"},"user_tz":-330},"outputId":"0e9877f8-eb94-401a-c80d-a6731bf280f0"},"source":["print(len(sent1.vocab))\n","print(len(sent2.vocab))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["20891\n","24185\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"q18dS44DTc0X"},"source":["# Building the model"]},{"cell_type":"code","metadata":{"id":"-vLGb7XZTXCr"},"source":["# Encoder\n","\n","class Encoder(nn.Module):\n","    def __init__(self, \n","                 input_dim, \n","                 hid_dim, \n","                 n_layers, \n","                 n_heads, \n","                 pf_dim,\n","                 dropout, \n","                 device,\n","                 max_length = 100):\n","        super().__init__()\n","\n","        self.device = device\n","        \n","        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n","\n","        # step added for custom embedding\n","        # loading the pre-trained embeddings into the model \n","        self.tok_embedding.weight.data.copy_(sent1.vocab.vectors)\n","\n","        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n","        \n","        self.layers = nn.ModuleList([EncoderLayer(hid_dim, \n","                                                  n_heads, \n","                                                  pf_dim,\n","                                                  dropout, \n","                                                  device) \n","                                     for _ in range(n_layers)])\n","        \n","        self.dropout = nn.Dropout(dropout)\n","        \n","        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n","        \n","    def forward(self, src, src_mask):\n","        \n","        #src = [batch size, src len]\n","        #src_mask = [batch size, src len]\n","        \n","        batch_size = src.shape[0]\n","        src_len = src.shape[1]\n","        \n","        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n","        \n","        #pos = [batch size, src len]\n","        \n","        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n","        \n","        #src = [batch size, src len, hid dim]\n","        \n","        for layer in self.layers:\n","            src = layer(src, src_mask)\n","            \n","        #src = [batch size, src len, hid dim]\n","            \n","        return src"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"llINpMvaTiEe"},"source":["# Encoder Layer\n","\n","class EncoderLayer(nn.Module):\n","    def __init__(self, \n","                 hid_dim, \n","                 n_heads, \n","                 pf_dim,  \n","                 dropout, \n","                 device):\n","        super().__init__()\n","        \n","        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n","        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n","        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n","        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n","                                                                     pf_dim, \n","                                                                     dropout)\n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def forward(self, src, src_mask):\n","        \n","        #src = [batch size, src len, hid dim]\n","        #src_mask = [batch size, src len]\n","                \n","        #self attention\n","        _src, _ = self.self_attention(src, src, src, src_mask)\n","        \n","        #dropout, residual connection and layer norm\n","        src = self.self_attn_layer_norm(src + self.dropout(_src))\n","        \n","        #src = [batch size, src len, hid dim]\n","        \n","        #positionwise feedforward\n","        _src = self.positionwise_feedforward(src)\n","        \n","        #dropout, residual and layer norm\n","        src = self.ff_layer_norm(src + self.dropout(_src))\n","        \n","        #src = [batch size, src len, hid dim]\n","        \n","        return src"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5sH1GLNWTneU"},"source":["# MultiHead Attention Layer\n","\n","class MultiHeadAttentionLayer(nn.Module):\n","    def __init__(self, hid_dim, n_heads, dropout, device):\n","        super().__init__()\n","        \n","        assert hid_dim % n_heads == 0\n","        \n","        self.hid_dim = hid_dim\n","        self.n_heads = n_heads\n","        self.head_dim = hid_dim // n_heads\n","        \n","        self.fc_q = nn.Linear(hid_dim, hid_dim)\n","        self.fc_k = nn.Linear(hid_dim, hid_dim)\n","        self.fc_v = nn.Linear(hid_dim, hid_dim)\n","        \n","        self.fc_o = nn.Linear(hid_dim, hid_dim)\n","        \n","        self.dropout = nn.Dropout(dropout)\n","        \n","        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n","        \n","    def forward(self, query, key, value, mask = None):\n","        \n","        batch_size = query.shape[0]\n","        \n","        #query = [batch size, query len, hid dim]\n","        #key = [batch size, key len, hid dim]\n","        #value = [batch size, value len, hid dim]\n","                \n","        Q = self.fc_q(query)\n","        K = self.fc_k(key)\n","        V = self.fc_v(value)\n","        \n","        #Q = [batch size, query len, hid dim]\n","        #K = [batch size, key len, hid dim]\n","        #V = [batch size, value len, hid dim]\n","                \n","        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n","        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n","        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n","        \n","        #Q = [batch size, n heads, query len, head dim]\n","        #K = [batch size, n heads, key len, head dim]\n","        #V = [batch size, n heads, value len, head dim]\n","                \n","        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n","        \n","        #energy = [batch size, n heads, query len, key len]\n","        \n","        if mask is not None:\n","            energy = energy.masked_fill(mask == 0, -1e10)\n","        \n","        attention = torch.softmax(energy, dim = -1)\n","                \n","        #attention = [batch size, n heads, query len, key len]\n","                \n","        x = torch.matmul(self.dropout(attention), V)\n","        \n","        #x = [batch size, n heads, query len, head dim]\n","        \n","        x = x.permute(0, 2, 1, 3).contiguous()\n","        \n","        #x = [batch size, query len, n heads, head dim]\n","        \n","        x = x.view(batch_size, -1, self.hid_dim)\n","        \n","        #x = [batch size, query len, hid dim]\n","        \n","        x = self.fc_o(x)\n","        \n","        #x = [batch size, query len, hid dim]\n","        \n","        return x, attention"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7IIpWl13Tvdt"},"source":["# Position-wise Feedforward Layer\n","\n","class PositionwiseFeedforwardLayer(nn.Module):\n","    def __init__(self, hid_dim, pf_dim, dropout):\n","        super().__init__()\n","        \n","        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n","        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n","        \n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def forward(self, x):\n","        \n","        #x = [batch size, seq len, hid dim]\n","        \n","        x = self.dropout(torch.relu(self.fc_1(x)))\n","        \n","        #x = [batch size, seq len, pf dim]\n","        \n","        x = self.fc_2(x)\n","        \n","        #x = [batch size, seq len, hid dim]\n","        \n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5QYWS6KtT7Pv"},"source":["# Decoder\n","\n","class Decoder(nn.Module):\n","    def __init__(self, \n","                 output_dim, \n","                 hid_dim, \n","                 n_layers, \n","                 n_heads, \n","                 pf_dim, \n","                 dropout, \n","                 device,\n","                 max_length = 100):\n","        super().__init__()\n","        \n","        self.device = device\n","        \n","        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n","        \n","        # step added for custom embedding\n","        self.tok_embedding.weight.data.copy_(sent2.vocab.vectors)\n","\n","        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n","        \n","        self.layers = nn.ModuleList([DecoderLayer(hid_dim, \n","                                                  n_heads, \n","                                                  pf_dim, \n","                                                  dropout, \n","                                                  device)\n","                                     for _ in range(n_layers)])\n","        \n","        self.fc_out = nn.Linear(hid_dim, output_dim)\n","        \n","        self.dropout = nn.Dropout(dropout)\n","        \n","        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n","        \n","    def forward(self, trg, enc_src, trg_mask, src_mask):\n","        \n","        #trg = [batch size, trg len]\n","        #enc_src = [batch size, src len, hid dim]\n","        #trg_mask = [batch size, trg len]\n","        #src_mask = [batch size, src len]\n","                \n","        batch_size = trg.shape[0]\n","        trg_len = trg.shape[1]\n","        \n","        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n","                            \n","        #pos = [batch size, trg len]\n","            \n","        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n","                \n","        #trg = [batch size, trg len, hid dim]\n","        \n","        for layer in self.layers:\n","            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n","        \n","        #trg = [batch size, trg len, hid dim]\n","        #attention = [batch size, n heads, trg len, src len]\n","        \n","        output = self.fc_out(trg)\n","        \n","        #output = [batch size, trg len, output dim]\n","            \n","        return output, attention"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lkOwWahOT-11"},"source":["# Decoder Layer\n","\n","class DecoderLayer(nn.Module):\n","    def __init__(self, \n","                 hid_dim, \n","                 n_heads, \n","                 pf_dim, \n","                 dropout, \n","                 device):\n","        super().__init__()\n","        \n","        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n","        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\n","        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n","        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n","        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n","        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n","                                                                     pf_dim, \n","                                                                     dropout)\n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def forward(self, trg, enc_src, trg_mask, src_mask):\n","        \n","        #trg = [batch size, trg len, hid dim]\n","        #enc_src = [batch size, src len, hid dim]\n","        #trg_mask = [batch size, trg len]\n","        #src_mask = [batch size, src len]\n","        \n","        #self attention\n","        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n","        \n","        #dropout, residual connection and layer norm\n","        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n","            \n","        #trg = [batch size, trg len, hid dim]\n","            \n","        #encoder attention\n","        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n","        \n","        #dropout, residual connection and layer norm\n","        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n","                    \n","        #trg = [batch size, trg len, hid dim]\n","        \n","        #positionwise feedforward\n","        _trg = self.positionwise_feedforward(trg)\n","        \n","        #dropout, residual and layer norm\n","        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n","        \n","        #trg = [batch size, trg len, hid dim]\n","        #attention = [batch size, n heads, trg len, src len]\n","        \n","        return trg, attention"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"drI5m5Z9UEaQ"},"source":["# Seq2seq\n","\n","class Seq2Seq(nn.Module):\n","    def __init__(self, \n","                 encoder, \n","                 decoder, \n","                 src_pad_idx, \n","                 trg_pad_idx, \n","                 device):\n","        super().__init__()\n","        \n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.src_pad_idx = src_pad_idx\n","        self.trg_pad_idx = trg_pad_idx\n","        self.device = device\n","        \n","    def make_src_mask(self, src):\n","        \n","        #src = [batch size, src len]\n","        \n","        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n","\n","        #src_mask = [batch size, 1, 1, src len]\n","\n","        return src_mask\n","    \n","    def make_trg_mask(self, trg):\n","        \n","        #trg = [batch size, trg len]\n","        \n","        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n","        \n","        #trg_pad_mask = [batch size, 1, 1, trg len]\n","        \n","        trg_len = trg.shape[1]\n","        \n","        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n","        \n","        #trg_sub_mask = [trg len, trg len]\n","            \n","        trg_mask = trg_pad_mask & trg_sub_mask\n","        \n","        #trg_mask = [batch size, 1, trg len, trg len]\n","        \n","        return trg_mask\n","\n","    def forward(self, src, trg):\n","        \n","        #src = [batch size, src len]\n","        #trg = [batch size, trg len]\n","                \n","        src_mask = self.make_src_mask(src)\n","        trg_mask = self.make_trg_mask(trg)\n","        \n","        #src_mask = [batch size, 1, 1, src len]\n","        #trg_mask = [batch size, 1, trg len, trg len]\n","        \n","        enc_src = self.encoder(src, src_mask)\n","        \n","        #enc_src = [batch size, src len, hid dim]\n","                \n","        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n","        \n","        #output = [batch size, trg len, output dim]\n","        #attention = [batch size, n heads, trg len, src len]\n","        \n","        return output, attention"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"omPx_mSBUI1Q"},"source":["# Training the model\n","\n","\n","INPUT_DIM = len(sent1.vocab)\n","OUTPUT_DIM = len(sent2.vocab)\n","HID_DIM = 300\n","ENC_LAYERS = 3\n","DEC_LAYERS = 3\n","ENC_HEADS = 10\n","DEC_HEADS =10\n","ENC_PF_DIM = 512\n","DEC_PF_DIM = 512\n","ENC_DROPOUT = 0.1\n","DEC_DROPOUT = 0.1\n","\n","enc = Encoder(INPUT_DIM, \n","              HID_DIM, \n","              ENC_LAYERS, \n","              ENC_HEADS, \n","              ENC_PF_DIM, \n","              ENC_DROPOUT, \n","              device)\n","\n","dec = Decoder(OUTPUT_DIM, \n","              HID_DIM, \n","              DEC_LAYERS, \n","              DEC_HEADS, \n","              DEC_PF_DIM, \n","              DEC_DROPOUT, \n","              device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QyIGjU0MUNHE"},"source":["SENT1_PAD_IDX = sent1.vocab.stoi[sent1.pad_token]  #Vocab.stoi â€“ A collections.defaultdict instance mapping token strings to numerical identifiers\n","SENT2_PAD_IDX = sent2.vocab.stoi[sent2.pad_token]\n","\n","model = Seq2Seq(enc, dec, SENT1_PAD_IDX, SENT2_PAD_IDX, device).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kk_yQ2_vUQu3","executionInfo":{"elapsed":692,"status":"ok","timestamp":1615454339107,"user":{"displayName":"ninja turtle","photoUrl":"","userId":"08410201356610850962"},"user_tz":-330},"outputId":"c027acb3-cd41-48bb-b6ee-016fcca4c3c6"},"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'The model has {count_parameters(model):,} trainable parameters')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The model has 25,970,357 trainable parameters\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hmjw9X1nUUzm"},"source":["def initialize_weights(m):\n","    if hasattr(m, 'weight') and m.weight.dim() > 1:\n","        nn.init.xavier_uniform_(m.weight.data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tKNxKuFvUWsU"},"source":["model.apply(initialize_weights);"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9mV1DS6kUaXo"},"source":["LEARNING_RATE = 0.0005\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bHZnN4aBUcZk"},"source":["criterion = nn.CrossEntropyLoss(ignore_index = SENT2_PAD_IDX)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8zH3YkwCUeGb"},"source":["def train(model, iterator, optimizer, criterion, clip):\n","    \n","    model.train()\n","    \n","    epoch_loss = 0\n","    \n","    for i, batch in enumerate(iterator):\n","        \n","        #src = batch.src\n","        #trg = batch.trg\n","        sent1 = batch.sent1\n","        sent2 = batch.sent2\n","        \n","        optimizer.zero_grad()\n","        \n","        output, _ = model(sent1, sent2[:,:-1])\n","                \n","        #output = [batch size, trg len - 1, output dim]\n","        #trg = [batch size, trg len]\n","            \n","        output_dim = output.shape[-1]\n","            \n","        output = output.contiguous().view(-1, output_dim)\n","        sent2 = sent2[:,1:].contiguous().view(-1)\n","                \n","        #output = [batch size * trg len - 1, output dim]\n","        #trg = [batch size * trg len - 1]\n","            \n","        loss = criterion(output, sent2)\n","        \n","        loss.backward()\n","        \n","        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","        \n","        optimizer.step()\n","        \n","        epoch_loss += loss.item()\n","        \n","    return epoch_loss / len(iterator)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WXA_Y14wUfoY"},"source":["def evaluate(model, iterator, criterion):\n","    \n","    model.eval()\n","    \n","    epoch_loss = 0\n","    \n","    with torch.no_grad():\n","    \n","        for i, batch in enumerate(iterator):\n","\n","            sent1 = batch.sent1\n","            sent2 = batch.sent2\n","\n","            output, _ = model(sent1, sent2[:,:-1])\n","            \n","            #output = [batch size, trg len - 1, output dim]\n","            #trg = [batch size, trg len]\n","            \n","            output_dim = output.shape[-1]\n","            \n","            output = output.contiguous().view(-1, output_dim)\n","            sent2 = sent2[:,1:].contiguous().view(-1)\n","            \n","            #output = [batch size * trg len - 1, output dim]\n","            #trg = [batch size * trg len - 1]\n","            \n","            loss = criterion(output, sent2)\n","\n","            epoch_loss += loss.item()\n","        \n","    return epoch_loss / len(iterator)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9Y16L7NPUiIJ"},"source":["def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"msoT-LETUlAq","outputId":"5c2c761a-238b-4365-87f2-f2c89c9e76f2"},"source":["N_EPOCHS = 5\n","CLIP = 1\n","\n","best_valid_loss = float('inf')\n","\n","for epoch in range(N_EPOCHS):\n","    \n","    start_time = time.time()\n","    \n","    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n","    valid_loss = evaluate(model, valid_iterator, criterion)\n","    \n","    end_time = time.time()\n","    \n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    \n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'transformer-model.pt')\n","    \n","    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch: 01 | Time: 48m 29s\n","\tTrain Loss: 5.906 | Train PPL: 367.206\n","\t Val. Loss: 5.326 |  Val. PPL: 205.673\n","Epoch: 02 | Time: 49m 23s\n","\tTrain Loss: 5.068 | Train PPL: 158.779\n","\t Val. Loss: 5.155 |  Val. PPL: 173.328\n","Epoch: 03 | Time: 49m 59s\n","\tTrain Loss: 4.720 | Train PPL: 112.126\n","\t Val. Loss: 5.114 |  Val. PPL: 166.337\n","Epoch: 04 | Time: 49m 38s\n","\tTrain Loss: 4.429 | Train PPL:  83.819\n","\t Val. Loss: 5.125 |  Val. PPL: 168.116\n","Epoch: 05 | Time: 49m 38s\n","\tTrain Loss: 4.160 | Train PPL:  64.046\n","\t Val. Loss: 5.160 |  Val. PPL: 174.146\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"X9NwYT22Unul"},"source":[""],"execution_count":null,"outputs":[]}]}